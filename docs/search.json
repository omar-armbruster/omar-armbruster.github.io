[
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "Abstract\nMachine learning methods provide the opportunity to extract features from a dataset that would otherwise be hidden. In this blog post, we examine Linear Regression and Random Forest Classifier models and train them on the Palmer Penguins dataset to show that we can reliably identify which of three species a penguin belongs to based on three of their characteristics. Using an exhaustive search, we identify that using sex, culmen length, and clumen depth, we can predict a penguin’s species with at least a 98% accuracy.\n\n\nLoading the data\nIn order to begin our deep dive into the Palmer Penguins dataset, we must first load the data into our python notebook, which we achieve with the read_csv function from the pandas library. As we will see momentarily, the species labels contain the english and latin names for the penguins. For the sake of simplicity, we extract the first word in each species label, leaving us with three simple species labels: Chinstrap, Gentoo, and Adelie.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\nIt’s always a good idea to get an idea of what the data looks like before diving into the analysis. Based on this snapshot of the data, we can start considering which qualitative and quantitative variables may be useful to us when fitting our models. Features like “studyName”, “Sample Number”, and “Individual ID” are likely not useful features (since they have nothing to do with the natural features of a penguin), but it may be worth looking at the other qualitative features like “Region”, “Island”, “Clutch Completion”, or “Sex”. Quantitative features like “Culmen Length”, “Culmen Depth”, and “Flipper Length” will also likely be useful.\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\nExploratory data analysis\nBefore we jump straight into model fitting, it’s a good idea to stop for a moment (Freeze if you will) and get an idea of which features will be most useful to our models. There are a variety of outcomes (an ocean of possibility perhaps?) that can arise based on which combinations of features we choose, so by examining the relationships between features of the data, we can get an idea for which features will produce the highest predictive accuracy. Using the matplotlib and seaborn packages, we can visualize our data, which may provide us with some of these insights.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\nOur scatterplot examines two qualitative variables: “Delta 13 C” and “Delta 15 N”, which are isotopes that were found in the blood of the Palmer Penguins (Don’t ever say that data science is easy; those penguins put their blood sweat and tears into this dataset!). Based on the scatterplot, the two isotopes do provide some amount of clustering between each species. However, the Adelie region does appear to overlap quite a bit with the other two regions, meaning that we would likely not want to use these features without a third feature to distinguish between penguins in the overlapping regions.\nThe stack plot examines the quantity of penguins for each species on each of the three islands included in this study. It appears that Gentoo penguins only appear on Biscoe Island and Chinstrap penguins only appear on Dream Island, while Adelie penguins appear on all three islands. Based on this information, if we have the name of the island, we can determine if a penguin is not a Gentoo or not a Chinstrap. However, we cannot definitively determine which species the penguin is (unless it’s on Torgersen island where it’s likely an Adelie) without additional features.\n\nfig, ax = plt.subplots(1, 2, figsize = (7, 4))\n\np1 = sns.scatterplot(train, x = 'Delta 15 N (o/oo)', y = 'Delta 13 C (o/oo)', hue = 'Species', ax = ax[0])\np2 = so.Plot(train, x = 'Island', color = 'Species').add(so.Bar(), so.Count(), so.Stack())\np2.on(ax[1]).show()\n\n\n\n\n\n\n\n\nWe can also take a closer look at the quantitative variables by looking at the mean and standard deviation values of a variety of features for each species. If the mean is not within a standard deviation of the means of the other two species, then it is likely that that feature could be a distinguinshable trait. With all four of these features, it appears that at least two species are always within a standard deviation of one another, meaning none of these traits can be used alone to classify the penguins.\nBased on these obervations, it is likely that we will need to use some combination of features to fit an accurate model, as it appears that there is no feature or pair of features that can singlehandedly predict the species.\n\ntrain.groupby(\"Species\").aggregate({'Flipper Length (mm)': ['mean', 'std'],\n                                    'Culmen Length (mm)': ['mean', 'std'],\n                                    'Culmen Depth (mm)': ['mean', 'std'],\n                                    'Body Mass (g)' : ['mean', 'std']})\n\n\n\n\n\n\n\n\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\nBody Mass (g)\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n190.084034\n6.689962\n38.970588\n2.640729\n18.409244\n1.217039\n3718.487395\n462.657007\n\n\nChinstrap\n196.000000\n7.423419\n48.826316\n3.450234\n18.366667\n1.138033\n3743.421053\n407.423479\n\n\nGentoo\n216.752577\n5.933715\n47.073196\n2.737415\n14.914433\n1.003431\n5039.948454\n498.861461\n\n\n\n\n\n\n\n\n\nData Preparation\nOur data requires a bit of preparation before it can be fed into our machine learning models. Our models only process quantitative variables, so we need to first represent our qualitative variables as numbers. Using the LabelEncoder, we convert the species labels to ternary form such that each species of penguin is represented by a \\(0\\), \\(1\\), or \\(2\\). We drop the qualitative features that are dependent on the study, rather than an observation made of a penguin, which allows us to isolate the features that could be used to identify a penguin even if it came from a completely different study. Using get_dummies(), we convert the remaining qualitative variables to a set of dummy columns, which express each qualitative outcome as booleans. Our prepare_data function returns X_train: the data as a set of binaries and quantiative variables, and y_train: the ternary set of labels corresponding to each penguin in the study, which we can plug straight into our machine learning models!\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nFrom a brief examination of X_train, we can see that we successfully isolated the quantitative variables and converted the meaningful qualitative variables to a set of booleans!\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nModel and Feature Selection\nOur goal for our model is to select two quantitative features and a qualitative features that we can use to optimize the classification accuracy. For this analysis, we choose to do an exhaustive search, which tests the predictive accuracy of all combinations of one qualitative and two quantitative features and selects whichever combination scores the highest. Because we divided our qualitative features into dummy columns earlier, selecting qualitative features will involve using multiple qualitative columns (ex: using the “Island” feature will use three columns).\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\ncol_combs = []\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols \n    # print(cols)\n    col_combs.append(cols) \n\nWe first choose to examine the Logistic Regression model, which we use to determine which combinations of features score the highest. Iterating through the combinations of columns we created above, we can fit every possible combination of features to a Logistic Regression model and update our best score and best columns each time we find a model that works better than the previous best.\n\n%%capture\nfrom sklearn.linear_model import LogisticRegression\nbest_score = 0\nbest_cols = []\nbest_model = LogisticRegression()\nfor col in col_combs:\n    LR = LogisticRegression()\n    LR.fit(X_train[col], y_train)\n    new_score = LR.score(X_train[col], y_train)\n    if new_score &gt; best_score:\n        best_score = new_score\n        best_cols = col\n        best_model = LR\n\nOur exhaustive search method returns that the three best features for classification are “Culmen Length”, “Culmen Depth”, and “Sex”. When using these three features on our training dataset, we achieve a classification accuracy of \\(99.6\\)%!.\n\nprint(best_score, best_cols)\n\n0.99609375 ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\n\n\nThe goal with building a machine learning classifier is to be able to accurately classify data that we don’t have the labels for. While a \\(99.6\\)% accuracy is certaintly exciting, it is not as impressive when we consider that the model has already seen the data that it is making predictions for. To test how applicable the model will be to new data, we can employ cross-validation, which trains the model and scores its predictions based on different subsets of the training data. When we calculate the mean of each subset’s score, we find that the model has an average score of \\(98.4\\)%, meaning that the model will likely perform well on new data and is not subject to overfitting! We can also use this cross-validation score as a comparison to other machine learning models to determine which will be most effective.\n\n%%capture\nfrom sklearn.model_selection import cross_val_score\ncv_scores_LR = cross_val_score(LR, X_train[best_cols], y_train, cv = 5)\n\nFor the sake of fairness, let’s take a look at a second machine learning model to see if we can improve upon the results of our Logistic Regression model. We can fit a Random Forest Classifier model just like we do with Logisitic Regression and we find that it achieves \\(100\\)% accuracy on the training data set!\n\nfrom sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier()\nRF.fit(X_train[best_cols], y_train)\nRF.score(X_train[best_cols], y_train)\n\n1.0\n\n\n\ncv_scores_RF = cross_val_score(RF, X_train[best_cols], y_train, cv = 5)\ncv_scores_RF.mean()\n\n0.9765460030165913\n\n\n\ncv_scores_LR.mean()\n\n0.9844645550527904\n\n\nNow comes the moment of truth: seeing how well our models work on a new set of data. We import and prepare the data the same way we did with our training data, which we then pass into each model’s scoring method. We need to be careful not to refit the models on this data, as the whole idea is to test how well our previous fitting works on new data. As we see below, the Logistic Regression and Random Forest Classifier models score \\(100\\)% and \\(98.5\\)% accuracy, respectively. Success!\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\nX_test, y_test = prepare_data(test)\ntest_LR = best_model.score(X_test[best_cols], y_test)\ntest_RF = RF.score(X_test[best_cols], y_test)\nprint(test_LR, test_RF)\n\n1.0 0.9852941176470589\n\n\n\n\nVisualizing the Model\nUsing a decision regions plot, we can visualize how our model assigns its labels. We can plot the two quantitative variables against one another, which creates a coordinate system with each point on the graph representing an individual penguin. Our models seek to define boundaries between each cluster of points, which in theory should divide the data into groups based on species. Because of the slightly superior performance of the Logisitic Regression model, we will examine its decision regions for both its training and testing data.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nBelow are the decision regions for the training dataset, divided by “Sex”, our selected qualitative variable. As we can see, the Logistic Regression model divides the coordinate space into three regions, each which correspond to a penguin species. When provided with new data, the model identifies where in the coordinate space that the new data point falls, and assigns a label based on whichever region it falls in. It appears that there are a few points that cross over to a different species’ region, which would account for the less than perfect accuracy we observed earlier. While our first instinct may be to try and fix the model so that it achieves perfect accuracy, this is not necessarily the goal, as redrawing our lines to accomodate for outliers will likely cause overfitting that will not translate well to new data. As such, these regions indicate a very strong classifier!\n\nplot_regions(best_model, X_train[best_cols], y_train)\n\nC:\\Users\\omara\\AppData\\Local\\Temp\\ipykernel_24932\\697242165.py:53: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n\n\n\n\n\n\n\n\n\nBelow shows the same decision regions applied to the testing data, and as we would expect, the new data aligns very well with our trained regions. Success Again!\n\nplot_regions(best_model, X_test[best_cols], y_test)\n\nC:\\Users\\omara\\AppData\\Local\\Temp\\ipykernel_24932\\697242165.py:53: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n\n\n\n\n\n\n\n\n\nAs a final check, it helps to generate a confusion matrix, which compares the real labels to the predicted labels to show where the classifier is making mistakes. For example, we may notice that the model tends to sometimes classify Adelie penguins as Gentoo penguins but never Gentoo penguins as Adelie penguins. This would perhaps help us identify similar features between the two or would maybe indicate that our model needs more training to be able to distinguish between the two. In our case, our model is fairly accurate, so it is a little tricky to make any strong conclusions about the errors in the model. Based on the confusion matrix for the training data below, it appears that one of the Chinstrap penguins was classified as a Gentoo penguin.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_train_pred  = best_model.predict(X_train[best_cols])\nC = confusion_matrix(y_train, y_train_pred)\nC\n\narray([[108,   0,   0],\n       [  1,  55,   0],\n       [  0,   0,  92]], dtype=int64)\n\n\n\n\nDiscussion\nUsing our machine learning models, we show that we can learn trends in the data and use them to classify unlabeled data that the models haven’t seen before. This can be a powerful tool as enabling machines to make educated decisions can allow for huge bounds in automation as well as allow for the indentification of trends that we would not be able to identify on our own. Based on the results above, it is possible to achieve optimal and effective clustering with only a few features, demonstrating how power and potential of these models."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Omar Armbruster’s Machine Learning Blog",
    "section": "",
    "text": "Palmer Penguins\n\n\n\n\n\nA Positively Preposterous Post Pondering Palmer Penguins\n\n\n\n\n\nFeb 12, 2025\n\n\nOmar Armbruster\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Maybe we were the ones learning all along…"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  }
]