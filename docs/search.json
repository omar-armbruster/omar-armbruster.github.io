[
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "The Perceptron",
    "section": "",
    "text": "The perceptron algorithm is the simplest model of a neural network, designed to take multiple inputs and output a single output representing the class of the input in a binary classification problem. The model works by iteratively updating a set of weights until a loss of zero is achieved, indicating perfect classification accuracy. We implement the perceptron algorithm and test it on linearly separable, non-linearly separable, and multi-dimensional linearly separable data to test its capabilities. We also implement a minibatch version of the algorithm, which examines multiple rows of the data at each step and updates the weights using their average gradient. Using these experiments, we can determine the strengths and weaknesses of the perceptron algorithm, which we can use to inform improvements we make when developing further algorithms."
  },
  {
    "objectID": "posts/perceptron/index.html#the-gradient-function",
    "href": "posts/perceptron/index.html#the-gradient-function",
    "title": "The Perceptron",
    "section": "The Gradient Function",
    "text": "The Gradient Function\nThe learning capabilities of our perceptron largely occurs in the grad() function, where a gradient is calculated to update the weights of the algorithm (and thus improve the classification threshold). For each iteration of our algorithm, we pass a single row of the input data and its corresponding label into the algorithm. The algorithm calculates the update to the weights as such \\[\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\mathbf{1}[s_i(2y_i - 1) &lt; 0](2y_i - 1)\\mathbf{x}_i\\] where \\(i\\) represents the index of the row of data used and \\(s_i\\) represents the score of that row of data such that \\[s_i = \\langle w \\,, \\mathbf{x}_i\\rangle\\] with \\(w\\) representing the existing weights. At each step of the algorithm, the perceptron randomly selects an observation from the data and updates the weights if the observation is misclassified. Since our classifier selects groups based on if the score of a given observation is positive or negative, we can check if an observation is misclassified using the expression \\([s_i(2y_i - 1)&lt;0]\\). This expression converts each label \\(y\\) from values of \\(0\\) and \\(1\\) to \\(-1\\) and \\(1\\). This means that observations with negative scores should have a label of \\(-1\\) and observations with positive scores should have a label of \\(1\\). Because the product of two real numbers with opposite signs is always negative and the product of two real numbers with the same signs is positive, we will find that all correctly classified observations will have positive products when multiplying their scores and labels while misclassified observations will be negative. Thus, by evaluating the boolean expression above, we will multiply our gradient by \\(0\\) for correctly classified observations (no change is made to the weights) and \\(1\\) for misclassified observations. Our gradient is then determined by the values of the observation, which are added to the current weights if the real label is positive (\\(2y_i - 1\\) evaluates to \\(1\\)) and subtracted if the real label is negative."
  },
  {
    "objectID": "posts/perceptron/index.html#linearly-separable-data",
    "href": "posts/perceptron/index.html#linearly-separable-data",
    "title": "The Perceptron",
    "section": "1) Linearly Separable Data",
    "text": "1) Linearly Separable Data\nWe feed the same data from our initial trial into our new and improved perceptron and use the returned benchmark data to visualize the changes in the weights at each step where the dotted line represents the previous classification boundary and the solid line represents the updated boundary based on the selected point (shaded in black). The boundary improves at each step and while the loss does not always decrease, we can see that the orientation of the line continuously imrpoves indicating some sort of progress towards the optimal solution.\n\ntorch.manual_seed(18)\np, loss, ws, idxs = Percy_with_life360(X, y)\n# # set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nplot_progress(X, y, ws, loss, idxs, axarr)\nplt.tight_layout()\n\n\n\n\n\n\n\n\nWe also plot the loss for the entire training loop, which demonstrates that there is not always a direct improvement, largely because no changes are made any time that a correctly classified point is selected.\n\nplt.plot(loss)\nplt.title('Perceptron Loss on Linearly Separable Data')\nplt.ylabel('Loss')\nplt.xlabel('Iteration')\n\nText(0.5, 0, 'Iteration')"
  },
  {
    "objectID": "posts/perceptron/index.html#non-linearly-separable-data",
    "href": "posts/perceptron/index.html#non-linearly-separable-data",
    "title": "The Perceptron",
    "section": "2) Non-linearly Separable Data",
    "text": "2) Non-linearly Separable Data\nNow for a harder problem. Up to this point, there has always been a line that exists that could divide the two clusters with perfect accuracy. With this new data, this is no longer the case. We generate this data in the same way as the linearly separable data but increase the gaussian noise from \\(0.2\\) to \\(0.4\\), which causes the points from the two clusters to overlap.\n\ntorch.manual_seed(14)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nuX, uy = perceptron_data(300, 0.4, p_dims=2)\nplot_perceptron_data(uX, uy, ax)\n\n\n\n\n\n\n\n\nWhen visualizing Percy’s progress through the training loop, we notice that it never achieves a loss of \\(0\\). As we discussed earlier, this is because no solution exists that would achieve a perfect loss. Letting the perceptron algorithm run on this data will eventually achieve an optimal, but never perfect solution to the classification problem. In this case, we can achieve a very small loss, but it will never be \\(0\\). Because of the way the perceptron algorithm is designed, the training loop will continue indefinitely for this data, so it is important that we implemented the stopping condition which stops the training loop if there is no convergence after \\(1000\\) iterations.\n\ntorch.manual_seed(14)\np, loss, ws, idxs = Percy_with_life360(uX, uy)\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nplot_progress(uX, uy, ws, loss, idxs, axarr)\nplt.tight_layout()\n\n\n\n\n\n\n\n\nWhen we plot the loss, we can notice large rises and drops in the loss where each update to the weights are made. While we are able to get a fairly low loss (indicating high classification accuracy), we are required to cut off the training loop early since we know that the loop will never terminate. Because the loss does not converge, we can notice that the loss does not terminate at the lowest observed loss, revealing a weakness of the perceptron algorithm. Assigning an arbitrary cutoff like we did will cause the algorithm to cease at exactly \\(1000\\) iterations. This means we run the risk of stopping the algorithm in the middle of one of the large rises, meaning we don’t always get the lowest possible loss.\n\nplt.plot(loss)\nplt.title('Perceptron Loss Non-Linearly Separable Data')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\n\nText(0, 0.5, 'Loss')"
  },
  {
    "objectID": "posts/perceptron/index.html#dimensional-data",
    "href": "posts/perceptron/index.html#dimensional-data",
    "title": "The Perceptron",
    "section": "3) 8-Dimensional Data",
    "text": "3) 8-Dimensional Data\nOur final dataset contains eight dimensions instead of two, meaning we cannot visualize the change in the classification boundary as we did for the other datasets. However, we can still monitor the loss. When we do, we notice that the loss converges, indicating that the algorithm does in fact work for multi-dimensional data and that this particular dataset is linearly separable. Nice!\n\ntorch.manual_seed(16)\nmX, my = perceptron_data(300, 0.2, p_dims=8)\np, loss, ws, idxs = Percy_with_life360(mX, my)\nplt.plot(loss)\nplt.title('Perceptron Loss on 8-dimensional Data')\nplt.xlabel('Step')\nplt.ylabel('Loss')\n\nText(0, 0.5, 'Loss')"
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "This post examines the implementation of the logistic regression classifier. Using a linear model, we implement a gradient function that iteratively adjusts the weights used to score and classify a given dataset. We implement an additional term to our step function which allows for gradient descent with momentum. Using our implemented model, we perform a series of experiments that compare the efficiency of gradient descent and gradient descent with momentum, examine the effects of overfitting the model, and explore the efficacy of the model on empirical data. Based on these experiments, we can explore the capabilities and limits of logistic regression and gain a better understanding of gradient descent and one of its variations."
  },
  {
    "objectID": "posts/logistic-regression/index.html#vanilla-gradient-descent",
    "href": "posts/logistic-regression/index.html#vanilla-gradient-descent",
    "title": "Implementing Logistic Regression",
    "section": "Vanilla Gradient Descent",
    "text": "Vanilla Gradient Descent\nWe first perform logistic regression using vanilla gradient descent, meaning gradient descent where \\(\\beta = 0\\). We define the LogisticRegression model we implemented as well as the GradientDescentOptimizer, which will update the weights at each iteration. The training loop here is fairly simple, calling the step() function for a predetermined number of iterations. We also save the loss at each step so we can determine when the model has converged.\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n\nloss = []\nfor _ in range(3000):\n    l = LR.loss(X,y)\n    loss.append(l)\n    opt.step(X, y, alpha = 0.1, beta = 0)\n\nAfter \\(3000\\) iterations, we notice that the loss converges to a value close to \\(0\\). We can also observe that the our decision boundary (dictated by the optimal weights) perfectly divides the data into their correct clusters. Nice!\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 5))\nfig.suptitle('Logistic Regression')\nplot_results(LR, X, y, loss, ax)"
  },
  {
    "objectID": "posts/automated-decision/index.html",
    "href": "posts/automated-decision/index.html",
    "title": "Design and Impact of Automated Decisions",
    "section": "",
    "text": "Abstract\nThis post examines threshold-choosing techniques that allow us to evaluate the efficacy of a classifier outside of the accuracy. Our case examines bank lending where our machine learning model must evaluate if a borrower is likely to default on their loans. In this scenario, it is much more costly for the bank to approve a loan that will be defaulted on than to reject a borrower that would have actually been able to pay back their loan. Because the errors are asymmetric in cost, we can find an optimal threshold for approving and denying loans, which will allow the bank to maximize profits. We use logistic regression on our dataset to find an optimal set of weights and then use the dot product to use these weights to convert our data to a probability that each prosepctive borrower will default. Based on these probabilities, we test a set of thresholds for allowable default risk and select the optimal one that will maximize profit. We find that the optimal threshold rejects any prospective borrower with a risk of default above \\(58\\%\\) and results in a profit of roughly \\(\\$ 1400\\) per prospective borrower. We also analyze how our threshold impacts different demographics of borrowers.\n\n\nIntroduction\nTo loan or not to loan: That is the question which most often plagues the indecisive banker. For the siren call of unlimited riches in the form of monthly interest payments beckons, but can lead to certain doom for those who make hasty loans that end in default. So the banker is left with a choice: lay down anchor and decry all loans to avoid disaster or set sail into the perilous loan-giving waters in search of the glorious bounty that awaits. Fortunately for today’s bankers, our modern day Prometheus–Big Tech–has bestowed upon us machines, which can make decisions autonomously, providing some solace for those who do not wish to bear the decision-making responsibility alone.\nWe can train one such machine learning model as follows, beginning by importing our trusty tools NumPy and Pandas as well as our training dataset. Our interest rates are given as percentages, which we will want to convert to a decimal for later calculations.\n\nimport pandas as pd\nimport numpy as np\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train['loan_int_rate'] = df_train[\"loan_int_rate\"]/100 # Convert percentage to decimal\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n0.0991\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n0.1347\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n0.0751\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n0.1287\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n0.0963\n0\n0.28\nN\n10\n\n\n\n\n\n\n\nWe preprocess the data in a way that can be read by our machine by converting our categorical columns into dummy columns and dropping NA values. We also separate our target variable “loan_status”, which is a \\(1\\) if the borrower defaulted and a \\(0\\) if the borrower paid back their loan. “loan_grade” is a measurement from the bank that determines the likelyhood that the loan is paid back. We want to remove this as well, as the goal is for our model to not be under the influence of outside evaluation methods.\n\ndef preprocess_data(df):\n    df = df.dropna()\n    y_train = df[\"loan_status\"]\n    df = df.drop([\"loan_status\", \"loan_grade\"], axis = 1)\n    df = pd.get_dummies(df)\n    return df, y_train\n\nX_train, y_train = preprocess_data(df_train)\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n0.1347\n0.12\n6\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n0.0751\n0.27\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n0.1287\n0.05\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n0.0963\n0.28\n10\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n0.1491\n0.25\n2\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\n\nExploring The Data\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nOur model is only as good as the data it is trained on, and as such, it is important for us to gain a strong understanding of our data before beginning the training process. The scatterplot on the left examines the relationship between two continuous features: “loan_percent_income” and “loan_int_rate”, representing the percentage of the borrower’s income that the requested loan consitutes and the the interest rate of the loan, respectively. Based on the scatterplot–which is colored by if the borrower defaulted–it appears that as both variables increase, the rate of default also increases.\nThe barplot to the right examines the homeowner status of borrowers and the rate at which each of those groups default on their loans. Borrowers who were renters and had a status of “Other” had the highest rates of default at around \\(30\\%\\), while borrowers who had a mortgage or owned their homes had lower default rates of around \\(12\\%\\) and \\(9\\%\\), respectively.\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 4))\nplt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.3, hspace=0.4)\np1 = sns.scatterplot(df_train, x = \"loan_percent_income\", y = \"loan_int_rate\", hue = \"loan_status\", ax = ax[0])\np2 = sns.barplot(df_train, x = \"person_home_ownership\", y = \"loan_status\", ax = ax[1])\n\n\n\n\n\n\n\n\nUsing the table below, we can examine the outcomes for groups with varying intents for their loans. It appears that on average, the interest rate granted by the bank was around \\(11\\%\\), regardless of what the intent was for the loan. Venture loans had the lowest rate of default with a mean of \\(14.9\\%\\) while debt consolidation loans had the highest rate of default with a mean of \\(28.7\\%\\).\n\ndf_train.groupby(['loan_intent']).aggregate({'loan_int_rate': ['mean', 'std'],\n                                             'loan_status': ['mean', 'std']})\n\n\n\n\n\n\n\n\nloan_int_rate\nloan_status\n\n\n\nmean\nstd\nmean\nstd\n\n\nloan_intent\n\n\n\n\n\n\n\n\nDEBTCONSOLIDATION\n0.109833\n0.032782\n0.287458\n0.452631\n\n\nEDUCATION\n0.109655\n0.031817\n0.173396\n0.378626\n\n\nHOMEIMPROVEMENT\n0.111601\n0.033750\n0.264645\n0.441220\n\n\nMEDICAL\n0.110519\n0.032451\n0.263289\n0.440463\n\n\nPERSONAL\n0.110098\n0.032337\n0.193739\n0.395271\n\n\nVENTURE\n0.109409\n0.032190\n0.148678\n0.355809\n\n\n\n\n\n\n\n\n\nBuilding The Model\nThis is where the fun begins: we can now build a model that will help us make loans that optimize our profits. We’ve chosen ol’ reliable–Logistic Regression–as our model, which we can fit to a subset of the features in our data. “But which features?” you may be wondering. Great question! We can determine an ideal combination of features through a trial-and-error like process by fitting a model to different combinations of features and then performing cross-validation on each to determine which set will perform well on a new testing dataset. We define a function getCols() below to make sure all of our relevant categorical dummies are selected when choosing our features.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\ndef getCols(X, quant_cols, qual_cols):\n  cols = quant_cols\n  for qual in qual_cols: \n    expand_qual_cols = [col for col in X.columns if qual in col ]\n    cols = np.concatenate((cols, expand_qual_cols))\n  return cols\n\nOur first combination of features uses both of the categorical variables we examined in our plots above as well as the interest rate, percentage of the borrowers income that the loan constitutes, the borrower’s age, and the employment length of the borrower’s most recent occupation. Based on our cross-validation test, these features score \\(84.4\\%\\). Nice!\n\nqual_cols = ['person_home_ownership', 'loan_intent']\nquant_cols = ['loan_int_rate', 'loan_percent_income', 'person_age', 'person_emp_length']\ncols1 = getCols(X_train, quant_cols, qual_cols)\nLR = LogisticRegression()\nLR.fit(X_train[cols1], y_train)\ncross_val_score(LR, X_train[cols1], y_train, cv = 5).mean()\n\n0.8439336903961869\n\n\nOur second combination ignores all categorical variables and just focuses on the interest rate and the loan’s percentage of the borrower’s income, which could be effective based on the trend we saw in the scatterplot above. We find a cross-validation score of \\(82.6\\%\\). While not as high as the previous combination, the cross-validation score is quite high, indicating a strong predictive power between these two features.\n\nqual_cols = []\nquant_cols = ['loan_int_rate', 'loan_percent_income']\ncols2 = getCols(X_train, quant_cols, qual_cols)\nLR = LogisticRegression()\nLR.fit(X_train[cols2], y_train)\ncross_val_score(LR, X_train[cols2], y_train, cv = 5).mean()\n\n0.825773775136919\n\n\nOur last combination uses the same features as the first combination but ignores the loan interest rate and the loan’s percentage of the borrower’s income. Using cross-validation, we get a score of \\(78.5\\%\\).\n\nqual_cols = ['person_home_ownership', 'loan_intent']\nquant_cols = ['person_age', 'person_emp_length']\ncols3 = getCols(X_train, quant_cols, qual_cols)\nLR = LogisticRegression()\nLR.fit(X_train[cols3], y_train)\ncross_val_score(LR, X_train[cols3], y_train, cv = 5).mean()\n\n0.7849565667540506\n\n\nWhile accuracy is not the only consideration to be made when profitmaxxing (as we will get to in a moment), a higher accuracy and cross-validation score indicate a strength in our model that we will want when trying to minimize certain types of error. As such, we can select the first combination of features we tested, which include the loan intent, borrower’s homeowner status, the interest rate, the loan’s percentage of the borrower’s income, the borrower’s age, and the borrower’s employment history.\n\ncols = cols1\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.8458986336054481\n\n\nWith our trained model, we can extract the component of greatest interest (bank pun?): the weights, which indicate how much each feature should be considered. Using the weights, we can convert any set of borrower data into a set of probabilities representing the predicted probability that a given borrower will default on their loans.\n\nweights = LR.coef_[0]\nweights\n\narray([18.18778916,  9.26840527, -0.03010567, -0.0267996 , -0.66674985,\n       -0.4983932 , -1.81653014,  0.08564683, -0.04352733, -1.01633329,\n       -0.14003246, -0.1609493 , -0.6349797 , -0.90020427])\n\n\n\n\nOptimizing a Threshold\nUsing our calculate_norm_scores() function, we can calcuate the predicted probability \\(s\\) that each borrower in our dataset will default on their loans. This works by finding the dot product between the feature data and their corresponding weights and then using min-max scaling to set the dot products on a scale from \\(0\\) to \\(1\\). We can interpret these values as probabilities where a \\(1\\) represents a \\(100\\%\\) chance of defaulting and a \\(0\\) represents a \\(0\\%\\) chance of defaulting.\n\ndef calculate_norm_scores(X, weights):\n    unNorm = X@weights\n    return (unNorm - unNorm.min())/(unNorm.max() - unNorm.min())\ns = calculate_norm_scores(X_train[cols], weights)\n\nWe can plot the probabilities on a histogram to observe the distribution of the borrowers. The data resemble a gaussian where the average borrower appears to have around a \\(40\\%\\) chance of default.\n\n\nCode\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nhist = ax.hist(s, bins = 50, color = \"steelblue\", alpha = 0.6, linewidth = 1, edgecolor = \"black\")\nlabs = ax.set(xlabel = r\"Score $s$\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\nFigure 1: Histogram of scores. Code sourced from Phil Chodrow Lecture Notes.\n\n\n\n\n\nNow to choose our threshold. To find the optimal risk cutoff, we can employ the very elegant and efficient brute force approach. We can test \\(100\\) values of our threshold \\(t\\) between \\(0\\) and \\(1\\), calculating whether each borrower falls above or below the threshold. Borrowers who fall above the threshold are considered to be too high of a risk to give a loan to, while borrowers below the threshold will be approved to receive a loan. We can then compare these labels to the real labels from the “loan_status” column. Based on this, we can calculate the total profit of the bank. If the model predicts that a borrower will pay back their loan and the borrower actually does, the bank will make a profit of \\[loan\\_amnt*(1+0.25*int\\_rate)^{10} - loan\\_amnt\\] which assumes that the bank profits \\(25\\%\\) of the interest rate after paying operating expenses and that the loan is paid back in ten years. If the model predicts that the borrower will pay back their loans and they actually default, the bank’s profit can be calculated by \\[loan\\_amnt*(1 + 0.25*int\\_rate)^3 - 1.7*loan\\_amnt\\] which assumes that the borrower defaults after three years causing the bank to lose \\(70\\%\\) of the principal. This amount is typically a negative value. In the banking industry this is considered to be “bad for business”. We can sum the values of these two profits to determine the total profit of the bank. We can calculate the profit yielded by each threshold and then set our best threshold to be whichever yields the highest profit. In our figure below, it appears optimal to reject any borrowers with a risk of default higher than \\(58\\%\\), which yields a profit of \\(\\$1452.12\\) per prospective borrower. Not too bad!\n\n# Figure code sourced from Phil Chodrow Lecture Notes and modified for the algorithm used in this post.\n\nbest_benefit = 0\nbest_threshold = 0\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nfor t in np.linspace(0, 1, 101): \n    y_pred = s &gt;= t\n    tn = np.sum((X_train['loan_amnt']*(1 + 0.25*X_train['loan_int_rate'])**10 - X_train['loan_amnt'])[(y_train == 0)&(y_pred == 0)])\n    fn = np.sum((X_train['loan_amnt']*(1 + 0.25*X_train['loan_int_rate'])**3 - 1.7*X_train['loan_amnt'])[(y_train == 1)&(y_pred == 0)])\n    benefit = (tn + fn)/len(X_train)\n    ax.scatter(t, benefit, color = \"steelblue\", s = 10)\n    if benefit &gt; best_benefit: \n        best_benefit = benefit\n        best_threshold = t\n\nax.axvline(best_threshold, linestyle = \"--\", color = \"grey\", zorder = -10)\nlabs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Benefit per Prospective Borrower\", title = f\"Best Benefit of ${best_benefit:.2f} Per Prospective Borrower at Threshold t = {best_threshold:.3f}\")\n\n\n\n\n\n\n\n\n\n\nEvaluating The Model as a Banker\nSo how does our model perform on a testing dataset where the model weights have not been influenced by the data it is transforming? Let’s find out!\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test[\"loan_int_rate\"] = df_test[\"loan_int_rate\"]/100\nX_test, y_test = preprocess_data(df_test)\n\nUsing the same features we used before and the weights from our trained Logistic Regression model, we can transform the testing data into a set of probabilities as well.\n\ns = calculate_norm_scores(X_test[cols], weights)\n\nUsing the best threshold we found to be optimal from the training data, we can assign labels to each of our testing borrowers and calculate the bank’s total profit based on these predictions. We find that the bank profits \\(\\$1366.84\\) per prospective borrower, which is very similar to the profits we found with the training data. Success!\n\ny_pred = s &gt;= best_threshold\ntn = np.sum((X_test['loan_amnt']*(1 + 0.25*X_test['loan_int_rate'])**10 - X_test['loan_amnt'])[(y_test == 0)&(y_pred == 0)])\nfn = np.sum((X_test['loan_amnt']*(1 + 0.25*X_test['loan_int_rate'])**3 - 1.7*X_test['loan_amnt'])[(y_test == 1)&(y_pred == 0)])\nbenefit = (tn + fn)/len(X_test)\nbenefit\n\n1366.8358295680039\n\n\n\n\nEvaluating the Model as a Borrower\nWhile optimizing profits can certaintly be exhilerating, it does not come without its costs. Because the model cannot predict with complete certainty whether a borrower will default, there are bound to be borrowers who are denied loans even if they actually would have been able to pay back their loans. In the name of fairness, we can examine who was approved and denied loans and determine if certain groups of people are advantaged or disadvantaged by our model.\nWe first look at the average prediction based on the borrowers age. Our model predicts that borrowers from ages \\(20\\) to \\(30\\) will have the highest rate of default at \\(14.4\\%\\). While this does disadvantage young people, the model still predicts the rate of default to be much lower than the actual rate, meaning many young people that are approved a loan will still default. The greatest disparity in predictions is in the \\(50\\) to \\(60\\) year old age group, where only \\(2.9\\%\\) are predicted to default when this group actually has the highest rate at \\(28.9\\%\\).\n\ndf_test[\"y_pred\"] = y_pred\ndf_test.groupby(pd.cut(df_test[\"person_age\"], [20,30,40,50,60,80]))[[\"y_pred\", \"loan_status\"]].mean().reset_index()\n\n\n\n\n\n\n\n\nperson_age\ny_pred\nloan_status\n\n\n\n\n0\n(20, 30]\n0.143839\n0.223499\n\n\n1\n(30, 40]\n0.088584\n0.208466\n\n\n2\n(40, 50]\n0.054545\n0.202429\n\n\n3\n(50, 60]\n0.029412\n0.289474\n\n\n4\n(60, 80]\n0.083333\n0.461538\n\n\n\n\n\n\n\nNext we can look at the predictions for default based on loan intent. Just as with the age groups, the predicted rates of default are much lower than the actual rates of default, although by varying amounts. Among the highest predicted rates of default are medical expenses with a predicted rate of \\(19.6\\%\\) compared to the actual default rate of \\(28.2\\%\\). This means it is significantly harder to get approved for a medical loan than other loans like education or venture loans, which are given predicted default rates of \\(6.3\\%\\) and \\(8.4\\%\\) respectively.\n\ndf_test.groupby(\"loan_intent\")[[\"y_pred\", \"loan_status\"]].mean().reset_index()\n\n\n\n\n\n\n\n\nloan_intent\ny_pred\nloan_status\n\n\n\n\n0\nDEBTCONSOLIDATION\n0.201327\n0.279497\n\n\n1\nEDUCATION\n0.062925\n0.167421\n\n\n2\nHOMEIMPROVEMENT\n0.118506\n0.246088\n\n\n3\nMEDICAL\n0.195713\n0.281553\n\n\n4\nPERSONAL\n0.119238\n0.219227\n\n\n5\nVENTURE\n0.084025\n0.145701\n\n\n\n\n\n\n\nLastly, we can look at income brackets. Borrowers that make less than \\(\\$40000\\) are the most likely to default, which the model reflects by making it the most difficult for these borrowers to access credit. As the income brackets increase, the model predicts lower and lower rates of default even though the actual rates of default seem to level off above \\(\\$70000\\).\n\ndf_test[\"y_pred\"] = y_pred\ndf_test.groupby(pd.cut(df_test[\"person_income\"], [0,40000,70000,100000, 150000,1000000]))[[\"y_pred\", \"loan_status\"]].mean().reset_index()\n\n\n\n\n\n\n\n\nperson_income\ny_pred\nloan_status\n\n\n\n\n0\n(0, 40000]\n0.236352\n0.370569\n\n\n1\n(40000, 70000]\n0.121875\n0.198972\n\n\n2\n(70000, 100000]\n0.068262\n0.119625\n\n\n3\n(100000, 150000]\n0.012797\n0.105008\n\n\n4\n(150000, 1000000]\n0.00495\n0.115044\n\n\n\n\n\n\n\n\n\nConclusion\nThroughout this post, we examine automated decision algorithms, which take advantage of threshold setting to determine if borrowers should be approved loans or not. By assigning variable costs to different kinds of errors, we define our optimal model as one that does not necessarily minimize all error, but minimizes a specific kind of error. Using this approach, we find parameters that can be used to maximize the profit for our bank. One of the ethical concerns with implementing an algorithm like this is that certain demographs are likely to be assigned higher or lower risk scores simply based on the demographic they are part of. As such, certain groups can be unfairly advantaged or disadvantaged in decision-making processes like approving loans. In the case of the bank, one such concern is that people seeking medical loans have higher rates of default and thus are denied credit by the model at higher rates. This brings up the question: is this fair? From a profit standpoint, this certaintly is fair as the bank wants to minimize the risk they take on when giving credit and riskier investments should be taken on less frequently. However, from a humanitarian standpoint, this is an unfair policy as medical expenses are often for life-or-death procedures and it could be considered inhumane to deny the borrower the money they need for such a procedure even if they are perhaps unable to pay it back. Thus it comes down to how you define fairness: Is fairness based on equal exchanges and kept promises or is it our responsibility as humans to provide for one another even when there is no direct benefit to us? It seems that there are some decisions that even the best machines cannot make."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Omar Armbruster’s Machine Learning Blog",
    "section": "",
    "text": "Observing Double Descent\n\n\n\n\n\nSo you’re telling me my model is too athletic?\n\n\n\n\n\nApr 16, 2025\n\n\nOmar Armbruster\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nThose penguins won’t know what hit them\n\n\n\n\n\nApr 7, 2025\n\n\nOmar Armbruster\n\n\n\n\n\n\n\n\n\n\n\n\nThe Perceptron\n\n\n\n\n\nIt’s Alive! Awakening the Perceptron\n\n\n\n\n\nApr 2, 2025\n\n\nOmar Armbruster\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nAnalyzing Implicit Bias in Machine Learning Models\n\n\n\n\n\nMar 12, 2025\n\n\nOmar Armbruster\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decisions\n\n\n\n\n\nBorrowers v. Bankers: A case study in maximizing shareholder value\n\n\n\n\n\nMar 5, 2025\n\n\nOmar Armbruster\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins\n\n\n\n\n\nA Positively Preposterous Post Pondering Palmer Penguins\n\n\n\n\n\nFeb 12, 2025\n\n\nOmar Armbruster\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Maybe we were the ones learning all along…"
  },
  {
    "objectID": "posts/auditing-bias/index.html",
    "href": "posts/auditing-bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Machine learning models show potential to revolutionize automated decision-making processes and have already made their way into many of our daily lives through technologies like personalized recommendations, face identification, and self-driving cars. However, these models can also pose extreme dangers, as the models are only as good as the data they are trained on and can very easily reinforce existing injustices in our society. Even when certain traits like race are omitted from the training process, models can predict these traits from other features and still reinforce societal norms that we are trying to avoid. We demonstrate this by training a Decision Tree Classifier model on a dataset of responses to the 2018 Public Use Microdata Sample survey in New York and use it to predict whether an individual is likely to employed. We omit race from our training features and then audit our model to determine if it is truly independent from race. We examine three methods of fairness: calibration, error rate balance, and statistical parity, as well as examine feasible false negative and false positive rates.\nWe begin by importing the folktables package, which we use to import our data. We specify that we are examining responses from 2018 in the state of New York when fetching our data.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"NY\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\nDownloading data for 2018 1-Year person survey for NY...\n\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000012\n2\n1\n3802\n1\n36\n1013097\n145\n26\n...\n146\n146\n21\n24\n266\n263\n21\n146\n265\n144\n\n\n1\nP\n2018GQ0000040\n2\n1\n2702\n1\n36\n1013097\n43\n21\n...\n6\n42\n43\n7\n40\n6\n43\n40\n42\n6\n\n\n2\nP\n2018GQ0000060\n2\n1\n2001\n1\n36\n1013097\n88\n18\n...\n88\n163\n161\n162\n87\n12\n162\n88\n87\n88\n\n\n3\nP\n2018GQ0000081\n2\n1\n2401\n1\n36\n1013097\n109\n85\n...\n17\n15\n111\n107\n17\n196\n109\n200\n198\n111\n\n\n4\nP\n2018GQ0000103\n2\n1\n1400\n1\n36\n1013097\n83\n19\n...\n81\n12\n80\n154\n12\n80\n12\n83\n152\n154\n\n\n\n\n5 rows × 286 columns\n\n\n\nFor the sake of simplicity, we can select specific features to train our model on. We select the following:\n\nAGEP - Age\nSCHL - Educational attainment\nMAR - Marital status\nRELP - Relationship to reference person\nDIS - Disability\nESP - Employment status of parents\nCIT - Citizenship stats\nMIG - Mobility status\nMIL - Military service\nANC - Ancestry\nNATIVITY - Nativity\nDEAR - Hearing difficulty\nDEYE - Vision difficulty\nDREM - Cognitive difficulty\nSEX - Sex\nRAC1P - Race\nESR - Employment status\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n26\n21.0\n5\n17\n2\nNaN\n5\n1.0\n4.0\n1\n2\n2\n2\n2.0\n2\n1\n6.0\n\n\n1\n21\n20.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n1\n1\n1.0\n\n\n2\n18\n16.0\n5\n17\n2\nNaN\n2\n3.0\n4.0\n1\n1\n2\n2\n2.0\n2\n8\n6.0\n\n\n3\n85\n16.0\n2\n16\n1\nNaN\n1\n1.0\n4.0\n4\n1\n1\n2\n1.0\n2\n1\n6.0\n\n\n4\n19\n19.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n2.0\n2\n1\n1.0\n\n\n\n\n\n\n\nWe use the selected features to define our training dataset but exclude employment status (the target variable) and race, which we claim the model will be able to implicitly determine. We define a problem, which specifies our target and hidden features (employment and race).\nLooking briefly at our data, it appears we have \\(196,967\\) observations (people) measured with \\(15\\) features. We confirm that our created feature, label, and group objects all have the same length, as each entry should correspond to a single person.\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(196967, 15)\n(196967,)\n(196967,)\n\n\nAs is good machine learning practice, we define a testing and training dataset so we can later evaluate how our model performs on unseen data.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\nFor later analysis, we create a function that modifies our training and testing datasets, which only include the training features. We add both the group (race) and label (employment status) columns to the new dataset as well as modify encoded labels to labels that we can understand. For the sake of analyzing sufficiently large racial groups, we sort groups into White, African American, Asian, and Other.\n\nimport pandas as pd\ndef format_data(df, features_to_use, group, labels):\n    new_df = pd.DataFrame(df, columns = features_to_use)\n    new_df['group'] = group\n    new_df['employed'] = labels\n    new_df['SEX'] = new_df['SEX'].replace({1.0: 'Male', 2.0: 'Female'})\n    new_df['group'] = new_df['group'].replace({1.0: 'White', \n                                               2.0: 'African American',\n                                               3.0: 'Other Race',\n                                               4.0: 'Other Race',\n                                               5.0: 'Other Race',\n                                               6.0: 'Asian',\n                                               7.0: 'Other Race',\n                                               8.0: 'Other Race',\n                                               9.0: 'Other Race'})\n\n    return new_df\n\nA quick examination of the training data shows that each of our racial groups are in the same order of magnitude with the exception of the White group, which has significantly more people in it.\n\ndf_train = format_data(X_train, features_to_use, group_train, y_train)\ndf_train.groupby('group').size()\n\ngroup\nAfrican American     19200\nAsian                13633\nOther Race           13904\nWhite               110836\ndtype: int64\n\n\nUsing this dataframe, we can determine the population of the training dataset, which we find to be \\(157573\\) people.\n\npopulation = df_train.shape[0]\npopulation\n\n157573\n\n\nOf these individuals, roughly \\(46.5\\%\\) people are employed. This rate is likely so low because we included children in our data, the majority of whom are unemployed.\n\ndf_train['employed'].mean()\n\n0.4649400595279648\n\n\nWe can also analyze the proportion of each race in our population. As we saw in the population counts above, African Americans, Asians, and Other Racial groups each make up a similar amount of the population with \\(11.0\\%\\), \\(9.3\\%\\), and \\(7.9\\%\\) respectively, while White people make up the remaining \\(71.8\\%\\) of the population.\n\ndf_train[df_train['employed'] == 1].groupby('group').size()/df_train[df_train['employed'] == 1].shape[0]\n\ngroup\nAfrican American    0.109607\nAsian               0.092517\nOther Race          0.079277\nWhite               0.718599\ndtype: float64\n\n\nDespite variances in the size of each racial group, we observe roughly equal employment rates across groups such that \\(41.8\\%\\) of African Americans are employed, \\(49.7\\%\\) of Asians are employed, \\(47.5\\%\\) of Whites are employed, and \\(41.8\\%\\) of individuals in other racial groups are employed. These values will be important later for when we audit our model to determine how consistent the predicted employment rates are with reality.\n\ndf_train.groupby('group')['employed'].mean()\n\ngroup\nAfrican American    0.418229\nAsian               0.497176\nOther Race          0.417722\nWhite               0.474990\nName: employed, dtype: float64\n\n\nIt is also worth checking for intersectional trends by examining what percentage of people are employed grouped by sex as well as race. This can help us determine if race is the only major factor in disparities between groups or if there are other contributing factors. Based on the bar plot below, it appears that a greater proportion of men than women are employed with the exception of the African American group. This indicates that women within each racial group may receive further bias from the algorithm, as it is possible that it will predict unemployment for women at higher rates. This may be more prevalent than bias by race since sex is actually a feature that we give to the model wheras race is not.\n\nimport seaborn as sns\nimport seaborn.objects as so\n\nprop = (df_train[df_train['employed'] == 1].groupby(['group', 'SEX']).size()/df_train.groupby(['group', 'SEX']).size()).reset_index(name = 'p')\np1 = sns.barplot(data = prop,\n                 x = 'group',\n                 y = 'p',\n                 hue = 'SEX',\n                 palette='Set2')\np1.set(title = 'Intersection between race and sex', xlabel = 'Race', ylabel = 'Proportion of group employed')\n\n[Text(0.5, 1.0, 'Intersection between race and sex'),\n Text(0.5, 0, 'Race'),\n Text(0, 0.5, 'Proportion of group employed')]"
  },
  {
    "objectID": "posts/auditing-bias/index.html#by-group-measures",
    "href": "posts/auditing-bias/index.html#by-group-measures",
    "title": "Auditing Bias",
    "section": "By-Group Measures",
    "text": "By-Group Measures\nWe can now look at the same quantities as above but sorted by group. Starting with accuracy, we notice that the model seems to be fairly consistent in its predictions with the exception of Asians who are only predicted correctly \\(80.0\\%\\) of the time compares to the other groups who are predicted correctly roughly \\(82\\%\\) of the time.\n\ndf.groupby(\"group\")['correct_prediction'].mean()\n\ngroup\nAfrican American    0.820896\nAsian               0.795996\nOther Race          0.831117\nWhite               0.826760\nName: correct_prediction, dtype: float64\n\n\nWe calculate the positive and negative counts as by group as well, which we will need for our PPV, FPR, and FNR values.\n\ndf.groupby('group')[['TP', 'TN', 'FP', 'FN']].sum()\n\n\n\n\n\n\n\n\nTP\nTN\nFP\nFN\n\n\ngroup\n\n\n\n\n\n\n\n\nAfrican American\n1800\n2160\n574\n290\n\n\nAsian\n1539\n1165\n537\n156\n\n\nOther Race\n1290\n1648\n402\n195\n\n\nWhite\n11104\n11746\n2955\n1833\n\n\n\n\n\n\n\nCalculating the PPVs, we find that the values are fairly consistent with the White group having the highest value of \\(79.0\\%\\) and the Asian group having the lowest predictive value of \\(74.1\\%\\). This means that \\(79.0\\%\\) of the White people that are predicted to be employed are actually employed and \\(74.1\\%\\) of the Asian people that are predicted to be employed are actually employed.\n\nPPV = df.groupby('group')['TP'].sum() / (df.groupby('group')['TP'].sum() + df.groupby('group')['FP'].sum())\nPPV\n\ngroup\nAfrican American    0.758214\nAsian               0.741329\nOther Race          0.762411\nWhite               0.789814\ndtype: float64\n\n\nCalculating the FPR, we find that Asians are falsly predicted to be employed at a much higher rate than other groups with an FPR of \\(31.6\\%\\). The reamining groups all have FPR values of roughly \\(20\\%\\), indicating that our model may have a bias towards the Asian group.\n\nFPR = df.groupby('group')['FP'].sum()/(df.groupby('group')['FP'].sum() + df.groupby('group')['TN'].sum())\nFPR\n\ngroup\nAfrican American    0.209949\nAsian               0.315511\nOther Race          0.196098\nWhite               0.201007\ndtype: float64\n\n\nThe FNR rates closely resemble the trends seen in the FPR values, with each group having a value of roughly \\(13.5\\%\\) with the exception fo the Asian group, which has an FNR of \\(9.2\\%\\). This reinforces our observation that the model tends to overestimate the employment rate of the Asian group relative to the other groups.\n\nFNR = df.groupby('group')['FN'].sum()/(df.groupby('group')['FN'].sum() + df.groupby('group')['TP'].sum())\nFNR\n\ngroup\nAfrican American    0.138756\nAsian               0.092035\nOther Race          0.131313\nWhite               0.141687\ndtype: float64"
  },
  {
    "objectID": "posts/auditing-bias/index.html#bias-measures",
    "href": "posts/auditing-bias/index.html#bias-measures",
    "title": "Auditing Bias",
    "section": "Bias Measures",
    "text": "Bias Measures\nIn order to check for bias, we can evaluate our model based on three standards: calibration, error rate balance, and statistical parity.\nWe can first examine calibration. A model is well calibrated if the prediction rates are consistent with the actual rates across racial groups. If we were to exclude the Asian group, we could say that our model is approximately calibrated. However, the Asian group has a significantly higher prediction rate than actual rate of employment, indicating poor calibration.\n\n# Calibration\nemployed = df.groupby('group')['employed'].mean()\npredicted_employed = df.groupby('group')['pred_employed'].mean()\npd.concat((predicted_employed, employed), axis = 1, keys = ['Prediction', 'True Rate'])\n\n\n\n\n\n\n\n\nPrediction\nTrue Rate\n\n\ngroup\n\n\n\n\n\n\nAfrican American\n0.492123\n0.433250\n\n\nAsian\n0.611127\n0.498970\n\n\nOther Race\n0.478642\n0.420085\n\n\nWhite\n0.508684\n0.468087\n\n\n\n\n\n\n\nNext we examine error rate parity, which evaluates if the FPR and FNR values are consistent with one another across groups. As with the calibration, three of the groups are fairly consistent with their FPR and FNR values, while the Asian group has a substantially higher FPR and a substantially lower FNR, indicating a poor error rate parity as well.\n\n# Error Rate Parity\npd.concat((FPR, FNR), axis = 1, keys = ['FPR', 'FNR'])\n\n\n\n\n\n\n\n\nFPR\nFNR\n\n\ngroup\n\n\n\n\n\n\nAfrican American\n0.209949\n0.138756\n\n\nAsian\n0.315511\n0.092035\n\n\nOther Race\n0.196098\n0.131313\n\n\nWhite\n0.201007\n0.141687\n\n\n\n\n\n\n\nLastly we can check for statistical parity, which examines if the predicted employment rates between groups are equal to one another. A model that satisfies statistical parity would have an equal proportion of people predicted to be employed across each group. Our model also does not satisfy statistical parity, as the predicted rate of Asian employment is signficantly higher than the rate for the three other racial groups.\n\n# Statistical Parity \ndf.groupby('group')['pred_employed'].mean()\n\ngroup\nAfrican American    0.492123\nAsian               0.611127\nOther Race          0.478642\nWhite               0.508684\nName: pred_employed, dtype: float64\n\n\nBased on the paper “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.” by Alexandra Chouldechova, we can examine how fair our model has the potential to be by finding the optimal combinations of FNR and FPR rates. We can calculate the FPR using the FNR with the equation \\[FPR = \\frac{p}{1 - p}\\frac{1 - PPV}{PPV}(1 - FNR)\\] where \\(p\\) is the prevalence or actual employment rate. As an approximation to calibrate our results, we set the PPV to be the minimum PPV value between groups. In this case, we are only examining the White and Asian groups, so we use the Asian PPV as our value. We can add shading to the White line, which shows acceptable FPR values for the White group if we allow the the PPV of the White group to deviate by \\(\\delta = 0.5\\), \\(0.1\\), and \\(0.125\\) from the PPV of the Asian group. Based on the plot, if we desired to tune our classifier threshold so that the false positive rates were equal between races, we would need to increase the false negative rate of the Asian group to roughly \\(0.2\\). This is not a substantial change, indicating that we could achieve a fair outcome without much of a decrease in the accuracy of the model.\n\nimport matplotlib.pyplot as plt\nPPV_min = PPV[['Asian', 'White']].min()\np = df.groupby('group')['employed'].mean()\nFNR_calc = np.linspace(0, 1, 50)\nFPR_calc = lambda p,FNR, PPV_min: (p/(1-p)) * ((1 - PPV_min)/PPV_min)*(1 - FNR)\nFPRs = [FPR_calc(ps, FNR_calc, PPV_min) for ps in p]\nfor i, FPR in enumerate(FPRs):\n    if(p.index[i] in ['Asian', 'White']):\n        plt.plot(FNR_calc, FPR, label = p.index[i])\n        plt.scatter(FNR[i], FPR[i])\nplt.fill_between(FNR_calc, FPR_calc(p['White'], FNR_calc, PPV_min -0.05),FPR_calc(p['White'], FNR_calc, PPV_min +0.05), alpha = 0.1)\nplt.fill_between(FNR_calc, FPR_calc(p['White'], FNR_calc, PPV_min -0.1),FPR_calc(p['White'], FNR_calc, PPV_min +0.1), alpha = 0.2)\nplt.fill_between(FNR_calc, FPR_calc(p['White'], FNR_calc, PPV_min -0.125),FPR_calc(p['White'], FNR_calc, PPV_min +0.125), alpha = 0.4)\nplt.xlabel('False Negative Rate')\nplt.ylabel('False Positive Rate')\nplt.title('Feasible (FNR, FPR) Combinations')\nplt.legend()"
  },
  {
    "objectID": "posts/double-descent/index.html",
    "href": "posts/double-descent/index.html",
    "title": "Observing Double Descent",
    "section": "",
    "text": "Abstract\nAfter thorough investigation, scientists have concluded that repeatedly banging your head against a wall is not conducive to positive learning outcomes. Some studies have even gone on to conclude that such practices actually inhibit learning and development. However, some research has indicated that we may not be able to make such conclusions when it comes to teaching machines. In fact, such practices may allow us to achieve levels of accuracy previously thought to be impossible. It has been observed that as the number of features increases relative to the number of observations, the model will become more overfit and will thus have a higher testing loss. This testing loss will asymptotically approach the point where the number of features and observations are equivalent. Very counterintuitively, if we continue to make the loss worse by adding more features, we will ultimately reach a point where the loss begins to decrease again, plateauing at a testing loss lower than was previously observed. This phenomenon known as double descent is an extremely desirable outcome for training reliable machine learning models and is the reason why deep learning models are able to achieve such high levels of accuracy with so many parameters. This blog post examines the double descent phenomenon by implementing and overfitting a linear regression model. We apply this method to a set of corrupted images and observe this change in the loss as we vary the number of features used to fit the model.\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nIn order to test the effects of varying parameter sizes, we need to first define the RandomFeatures class, which selects \\(n\\) random features and passes them through either a sigmoid activation function given by \\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\] or a square activation function given by \\[square(x) = x^2.\\] Using this approach, we can choose a random set of features (to avoid having our analysis be biased towards a specific singular feature) that are transformed with one of the two defined activation functions.\n\ndef sig(x): \n    return 1/(1+torch.exp(-x))\n\ndef square(x): \n    return x**2\n\nclass RandomFeatures:\n    \"\"\"\n    Random sigmoidal feature map. This feature map must be \"fit\" before use, like this: \n\n    phi = RandomFeatures(n_features = 10)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n\n    model.fit(X_train_phi, y_train)\n    model.score(X_test_phi, y_test)\n\n    It is important to fit the feature map once on the training set and zero times on the test set. \n    \"\"\"\n\n    def __init__(self, n_features, activation = sig):\n        self.n_features = n_features\n        self.u = None\n        self.b = None\n        self.activation = activation\n\n    def fit(self, X):\n        self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n        self.b = torch.rand((self.n_features), dtype = torch.float64) \n\n    def transform(self, X):\n        return self.activation(X @ self.u + self.b)\n\n\n\nImplementing Linear Regression\nThe standard formula for the optimal weight vector in least squares is given by \\[\\hat{\\textbf{w}} = argmin_{\\textbf{w}}||\\textbf{Xw} - \\textbf{y}||^2,\\] which has the closed-form solution \\[\\hat{\\textbf{w}} = (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}\\] only when the number of data observations \\(n\\) is larger than the number of features \\(p\\). This is because if \\(p &gt; n\\), the invertible matrix theorem will be violated, making it impossible to find the inverse of \\((\\textbf{X}^T\\textbf{X})\\). When \\(p &gt; n\\), there are more columns of \\(\\textbf{X}\\) than rows, meaning that it is impossible for each of the columns to be linearly independent, which is a requirement by the invertible matrix theorem for a matrix to be invertible.\nAs such, this solution will not hold for our experiments, meaning we need to alter our definition for the weight vector in our implementation of linear regression. We do this using the formula \\[\\hat{\\textbf{w}} = \\textbf{X}^+\\textbf{y}\\] where \\(\\textbf{X}^+\\) is defined as the Moore-Penrose pseudoinverse of \\(\\textbf{X}\\). We accomplish this with the function torch.linalg.pinv() and use this to calculate our optimal weight vector in our MyLinearRegression implementation.\n\n%load_ext autoreload\n%autoreload 2\nfrom linear import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nSimple Data\nWe first examine the efficacy of our algorithm on a simple set of data, scattered randomly in a parabolic shape. Using our linear regression model, we expect to be able to fit these data quite accurately. We can first start by plotting the data to be fit, which is roughly parabolic as expected.\n\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\nplt.scatter(X, y, color='darkgrey', label='Data')\n\n\n\n\n\n\n\n\nWe can then define our training dataset using the RandomFeatures class as discussed above. We choose \\(10\\) features at random to feed to our model and transform them with the sigmoid activation function. We then define our linear regression model and our optimizer and fit the model on our feature data. Unlike previous algorithms, we do not need to solve the weights iteratively, as we can calculate the weight vector directly using the Moore-Penrose approach above.\n\nphi = RandomFeatures(n_features = 10)\nphi.fit(X)\nX_train_features = phi.transform(X)\n\nLR = MyLinearRegression()\nopt = OverParameterizedLinearRegressionOptimizer(LR)\nopt.fit(X_train_features, y)\n# now LR.w has the optimal weight vector \n\nWe can then define our predictions based on the calculated weights and plot them against the original data. The model appears to be a very good fit of the data. While the model cannot predict the values perfectly accurate, it appears that our predictions follow the same trend as the data and will make the correct predictions on average.\n\npred = LR.predict(X_train_features)\nplt.scatter(X, y, color='darkgrey', label='Data')\nplt.plot(X, pred, color = 'red', label = 'Prediction')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nCorrupted Data and Overfitting\nNow that we have confirmed our model is able to effectively fit a given dataset, we can examine the model’s performance on more difficult problems. For this experiment, we will examine corrupted images where sections of the image have been removed. The goal of our model will be to fill in this missing sections as accurately as possible. The original, uncorrupted image can be seen below.\n\nfrom sklearn.datasets import load_sample_images\nfrom sklearn.model_selection import train_test_split\nfrom scipy.ndimage import zoom\n\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X,.2) #decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower, cmap = 'gray_r')\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\nWe then define the corruption function, which will allow us to randomly remove sections of the image for our model to predict.\n\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\nAn example of the corrupted image can be seen below. Each gray square represents a missing section of the data for our linear regression model to predict.\n\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1, cmap = 'gray_r')\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\nUsing our method for corrupting images, we can create training and testing datasets. We specify \\(n\\_samples = 200\\), meaning each of our datasets will have \\(100\\) images in them. We then apply our corrupted_image() function to \\(200\\) versions of our image and split them randomly into training and testing data.\n\nn_samples = 200\n\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = torch.float64)\ny = torch.zeros(n_samples, dtype = torch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\nX = X.reshape(n_samples, -1)\n# X.reshape(n_samples, -1).size()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\nNow that we have our training and testing datasets, we can examine the performance of our linear regression model as the number of features varies. We fit and evaluate the model for \\(0-400\\) features and store the training and testing losses, which will allow us to visualize how the efficacy of the model changes as the number of features increases. Unlike our previous experiment, we use the square activation function, which we have found through trial-and-error to be more effective in demonstrating double descent.\n\nmax_feat = 401\nmse_train = []\nmse_test = []\nfor i in range(max_feat):\n    phi = RandomFeatures(n_features = i, activation = square)\n    phi.fit(X_train)\n    X_train_features = phi.transform(X_train)\n    X_test_features = phi.transform(X_test)\n    LR = MyLinearRegression()\n    opt = OverParameterizedLinearRegressionOptimizer(LR)\n    opt.fit(X_train_features, y_train)\n    tr_loss = LR.loss(X_train_features, y_train)\n    te_loss = LR.loss(X_test_features, y_test)\n    mse_train.append(tr_loss)\n    mse_test.append(te_loss)\n\nWe plot the training loss (left) and the testing loss (right) as a function of the number of features used. In the training loss plot, we observe that as the number of features increases the training loss decreases. This is to be expected, as a greater number of features means more weights are used, allowing for more degrees of freedom in our fitting function. However, this also means that the model is becoming more overfit, as the greater degrees of freedom will allow the function to fit to smaller trends in the data that exist because of noise and will not be present in the testing set. Interestingly enough, the loss drops significantly once we approach \\(p = 100\\) features, which is where the number of features is equal to the number of observations. While we would expect the loss to continue to decrease, this large jump in the loss is unexpected and indicates some change in the behavior of the model fit.\nOur suspicions are confirmed when we observe the testing loss. As the number of features increases to \\(p = 100\\), the testing loss increases, which we would expect because a more overfit model will not generalize as well to unique testing data. However, as we increase the number of features past \\(p = 100\\), the loss counterintiuitively begins to decrease, reaching values lower than we observed prior to \\(p = 100\\). This illustrates the double descent that we have been looking for. When we add enough features, the model becomes so overfit that it is no longer overfit and is actually more accurate than it was originally.\n\nfig, ax = plt.subplots(1,2, figsize = (10,4))\nfeat = np.linspace(0,max_feat - 1, max_feat)\nax[0].scatter(feat, mse_train, color = 'gray', s = 12)\nax[1].scatter(feat, mse_test, color = 'darkred', s = 12)\nlabs = ['training', 'testing']\nfor i in range(len(ax)):\n    ax[i].set_yscale('log')\n    ax[i].set_xlabel('Number of features')\n    ax[i].set_ylabel('Mean squared error ({})'.format(labs[i]))\n    ax[i].grid(True)\n    ax[i].axvline(100, color = 'black', ymax = 0.15, lw = 3)\n\n\n\n\n\n\n\n\nWe calculate the feature at which the minimum testing loss occurs using the code below. We find that the lowest testing loss is achieved when we use \\(373\\) features, inidicating that the most efficient model is actually achieved when the number of features exceeds the number of observations rather than when we keep the number of features small as was originally expected.\n\nfeat[np.where(np.array(mse_test) == np.array(mse_test).min())][0]\n\n373.0\n\n\n\n\nConclusion\nThrough our experiments with our implemented linear regression model, we are able to design a set of experiements to observe double descent in action. By creating a linear regression model that can handle overparameterization, we are able to increase the number of parameters to exceed the number of observations, which allows us to achieve testing loss values lower than any testing loss achieved by using fewer features than observations.\nWhile there is still some uncertainty as to why this works, it is likely that the large number of parameters allows the model to perfectly fit the training data while still having enough parameters left over to generalize the data and tune out the effects of noise (thus revealing the true general form of the data). This phenomenon is extremely advantageous for building effective models and has consequently caused deep learning to dominate the field of machine learning. Because more parameters are almost always better, large models are made more accurate not by improving their algorithms, but by increasing the computing power allocated to them and thus the number of parameters they are able to use.\nThis focus on hardware rather than software has a number of implications. As AI and advanced machine learning algorithms enter the mainstream and become commonplace in technologies like our phones, the minimum hardware requirements increase, often making it more expensive for consumers to access new technology. This also makes it difficult for smaller companies to contribute to AI/ML research, as the scalability of models is almost entirely determined by how many GPUs they can afford. There are also environmental concerns here – the more GPUs are needed to train a model, the more electricity and cooling power are needed to support them, thus putting a strain on our environmental resources. While double descent is a powerful feature of machine learning that can bring algorithms to new levels of accuracy and usefulness, it can also have negative effects (such as those listed above) and should be considered carefully when approaching machine learning reasearch."
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "Abstract\nMachine learning methods provide the opportunity to extract features from a dataset that would otherwise be hidden. In this blog post, we examine Linear Regression and Random Forest Classifier models and train them on the Palmer Penguins dataset to show that we can reliably identify which of three species a penguin belongs to based on three of their characteristics. Using an exhaustive search, we identify that using sex, culmen length, and clumen depth, we can predict a penguin’s species with at least a 98% accuracy.\n\n\nLoading the data\nIn order to begin our deep dive into the Palmer Penguins dataset, we must first load the data into our python notebook, which we achieve with the read_csv function from the pandas library. As we will see momentarily, the species labels contain the english and latin names for the penguins. For the sake of simplicity, we extract the first word in each species label, leaving us with three simple species labels: Chinstrap, Gentoo, and Adelie.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\nIt’s always a good idea to get an idea of what the data looks like before diving into the analysis. Based on this snapshot of the data, we can start considering which qualitative and quantitative variables may be useful to us when fitting our models. Features like “studyName”, “Sample Number”, and “Individual ID” are likely not useful features (since they have nothing to do with the natural features of a penguin), but it may be worth looking at the other qualitative features like “Region”, “Island”, “Clutch Completion”, or “Sex”. Quantitative features like “Culmen Length”, “Culmen Depth”, and “Flipper Length” will also likely be useful.\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\nExploratory data analysis\nBefore we jump straight into model fitting, it’s a good idea to stop for a moment (Freeze if you will) and get an idea of which features will be most useful to our models. There are a variety of outcomes (an ocean of possibility perhaps?) that can arise based on which combinations of features we choose, so by examining the relationships between features of the data, we can get an idea for which features will produce the highest predictive accuracy. Using the matplotlib and seaborn packages, we can visualize our data, which may provide us with some of these insights.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\nOur scatterplot examines two qualitative variables: “Delta 13 C” and “Delta 15 N”, which are isotopes that were found in the blood of the Palmer Penguins (Don’t ever say that data science is easy; those penguins put their blood sweat and tears into this dataset!). Based on the scatterplot, the two isotopes do provide some amount of clustering between each species. However, the Adelie region does appear to overlap quite a bit with the other two regions, meaning that we would likely not want to use these features without a third feature to distinguish between penguins in the overlapping regions.\nThe stack plot examines the quantity of penguins for each species on each of the three islands included in this study. It appears that Gentoo penguins only appear on Biscoe Island and Chinstrap penguins only appear on Dream Island, while Adelie penguins appear on all three islands. Based on this information, if we have the name of the island, we can determine if a penguin is not a Gentoo or not a Chinstrap. However, we cannot definitively determine which species the penguin is (unless it’s on Torgersen island where it’s likely an Adelie) without additional features.\n\nfig, ax = plt.subplots(1, 2, figsize = (7, 4))\n\np1 = sns.scatterplot(train, x = 'Delta 15 N (o/oo)', y = 'Delta 13 C (o/oo)', hue = 'Species', ax = ax[0])\np2 = so.Plot(train, x = 'Island', color = 'Species').add(so.Bar(), so.Count(), so.Stack())\np2.on(ax[1]).show()\n\n\n\n\n\n\n\n\nWe can also take a closer look at the quantitative variables by looking at the mean and standard deviation values of a variety of features for each species. If the mean is not within a standard deviation of the means of the other two species, then it is likely that that feature could be a distinguinshable trait. With all four of these features, it appears that at least two species are always within a standard deviation of one another, meaning none of these traits can be used alone to classify the penguins.\nBased on these obervations, it is likely that we will need to use some combination of features to fit an accurate model, as it appears that there is no feature or pair of features that can singlehandedly predict the species.\n\ntrain.groupby(\"Species\").aggregate({'Flipper Length (mm)': ['mean', 'std'],\n                                    'Culmen Length (mm)': ['mean', 'std'],\n                                    'Culmen Depth (mm)': ['mean', 'std'],\n                                    'Body Mass (g)' : ['mean', 'std']})\n\n\n\n\n\n\n\n\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\nBody Mass (g)\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n190.084034\n6.689962\n38.970588\n2.640729\n18.409244\n1.217039\n3718.487395\n462.657007\n\n\nChinstrap\n196.000000\n7.423419\n48.826316\n3.450234\n18.366667\n1.138033\n3743.421053\n407.423479\n\n\nGentoo\n216.752577\n5.933715\n47.073196\n2.737415\n14.914433\n1.003431\n5039.948454\n498.861461\n\n\n\n\n\n\n\n\n\nData Preparation\nOur data requires a bit of preparation before it can be fed into our machine learning models. Our models only process quantitative variables, so we need to first represent our qualitative variables as numbers. Using the LabelEncoder, we convert the species labels to ternary form such that each species of penguin is represented by a \\(0\\), \\(1\\), or \\(2\\). We drop the qualitative features that are dependent on the study, rather than an observation made of a penguin, which allows us to isolate the features that could be used to identify a penguin even if it came from a completely different study. Using get_dummies(), we convert the remaining qualitative variables to a set of dummy columns, which express each qualitative outcome as booleans. Our prepare_data function returns X_train: the data as a set of binaries and quantiative variables, and y_train: the ternary set of labels corresponding to each penguin in the study, which we can plug straight into our machine learning models!\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nFrom a brief examination of X_train, we can see that we successfully isolated the quantitative variables and converted the meaningful qualitative variables to a set of booleans!\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nModel and Feature Selection\nOur goal for our model is to select two quantitative features and a qualitative features that we can use to optimize the classification accuracy. For this analysis, we choose to do an exhaustive search, which tests the predictive accuracy of all combinations of one qualitative and two quantitative features and selects whichever combination scores the highest. Because we divided our qualitative features into dummy columns earlier, selecting qualitative features will involve using multiple qualitative columns (ex: using the “Island” feature will use three columns).\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\ncol_combs = []\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols \n    # print(cols)\n    col_combs.append(cols) \n\nWe first choose to examine the Logistic Regression model, which we use to determine which combinations of features score the highest. Iterating through the combinations of columns we created above, we can fit every possible combination of features to a Logistic Regression model and update our best score and best columns each time we find a model that works better than the previous best.\n\n%%capture\nfrom sklearn.linear_model import LogisticRegression\nbest_score = 0\nbest_cols = []\nbest_model = LogisticRegression()\nfor col in col_combs:\n    LR = LogisticRegression()\n    LR.fit(X_train[col], y_train)\n    new_score = LR.score(X_train[col], y_train)\n    if new_score &gt; best_score:\n        best_score = new_score\n        best_cols = col\n        best_model = LR\n\nOur exhaustive search method returns that the three best features for classification are “Culmen Length”, “Culmen Depth”, and “Sex”. When using these three features on our training dataset, we achieve a classification accuracy of \\(99.6\\)%!.\n\nprint(best_score, best_cols)\n\n0.99609375 ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\n\n\nThe goal with building a machine learning classifier is to be able to accurately classify data that we don’t have the labels for. While a \\(99.6\\)% accuracy is certaintly exciting, it is not as impressive when we consider that the model has already seen the data that it is making predictions for. To test how applicable the model will be to new data, we can employ cross-validation, which trains the model and scores its predictions based on different subsets of the training data. When we calculate the mean of each subset’s score, we find that the model has an average score of \\(98.4\\)%, meaning that the model will likely perform well on new data and is not subject to overfitting! We can also use this cross-validation score as a comparison to other machine learning models to determine which will be most effective.\n\n%%capture\nfrom sklearn.model_selection import cross_val_score\ncv_scores_LR = cross_val_score(LR, X_train[best_cols], y_train, cv = 5)\n\n\ncv_scores_LR.mean()\n\n0.9844645550527904\n\n\nFor the sake of fairness, let’s take a look at a second machine learning model to see if we can improve upon the results of our Logistic Regression model. We can fit a Random Forest Classifier model just like we do with Logisitic Regression and we find that it achieves \\(100\\)% accuracy on the training data set!\n\nfrom sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier()\nRF.fit(X_train[best_cols], y_train)\nRF.score(X_train[best_cols], y_train)\n\n1.0\n\n\n\ncv_scores_RF = cross_val_score(RF, X_train[best_cols], y_train, cv = 5)\ncv_scores_RF.mean()\n\n0.9765460030165913\n\n\nNow comes the moment of truth: seeing how well our models work on a new set of data. We import and prepare the data the same way we did with our training data, which we then pass into each model’s scoring method. We need to be careful not to refit the models on this data, as the whole idea is to test how well our previous fitting works on new data. As we see below, the Logistic Regression and Random Forest Classifier models score \\(100\\)% and \\(98.5\\)% accuracy, respectively. Success!\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\nX_test, y_test = prepare_data(test)\ntest_LR = best_model.score(X_test[best_cols], y_test)\ntest_RF = RF.score(X_test[best_cols], y_test)\nprint(test_LR, test_RF)\n\n1.0 0.9852941176470589\n\n\n\n\nVisualizing the Model\nUsing a decision regions plot, we can visualize how our model assigns its labels. We can plot the two quantitative variables against one another, which creates a coordinate system with each point on the graph representing an individual penguin. Our models seek to define boundaries between each cluster of points, which in theory should divide the data into groups based on species. Because of the slightly superior performance of the Logisitic Regression model, we will examine its decision regions for both its training and testing data.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nBelow are the decision regions for the training dataset, divided by “Sex”, our selected qualitative variable. As we can see, the Logistic Regression model divides the coordinate space into three regions, each which correspond to a penguin species. When provided with new data, the model identifies where in the coordinate space that the new data point falls, and assigns a label based on whichever region it falls in. It appears that there are a few points that cross over to a different species’ region, which would account for the less than perfect accuracy we observed earlier. While our first instinct may be to try and fix the model so that it achieves perfect accuracy, this is not necessarily the goal, as redrawing our lines to accomodate for outliers will likely cause overfitting that will not translate well to new data. As such, these regions indicate a very strong classifier!\n\nplot_regions(best_model, X_train[best_cols], y_train)\n\n\n\n\n\n\n\n\nBelow shows the same decision regions applied to the testing data, and as we would expect, the new data aligns very well with our trained regions. Success Again!\n\nplot_regions(best_model, X_test[best_cols], y_test)\n\n\n\n\n\n\n\n\nAs a final check, it helps to generate a confusion matrix, which compares the real labels to the predicted labels to show where the classifier is making mistakes. For example, we may notice that the model tends to sometimes classify Adelie penguins as Gentoo penguins but never Gentoo penguins as Adelie penguins. This would perhaps help us identify similar features between the two or would maybe indicate that our model needs more training to be able to distinguish between the two. In our case, our model is fairly accurate, so it is a little tricky to make any strong conclusions about the errors in the model. Based on the confusion matrix for the training data below, it appears that one of the Chinstrap penguins was classified as a Gentoo penguin.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_train_pred  = best_model.predict(X_train[best_cols])\nC = confusion_matrix(y_train, y_train_pred)\nC\n\narray([[108,   0,   0],\n       [  1,  55,   0],\n       [  0,   0,  92]], dtype=int64)\n\n\n\n\nDiscussion\nUsing our machine learning models, we show that we can learn trends in the data and use them to classify unlabeled data that the models haven’t seen before. This can be a powerful tool as enabling machines to make educated decisions can allow for huge bounds in automation as well as allow for the indentification of trends that we would not be able to identify on our own. Based on the results above, it is possible to achieve optimal and effective clustering with only a few features, demonstrating how power and potential of these models."
  }
]