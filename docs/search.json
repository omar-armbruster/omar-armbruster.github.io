[
  {
    "objectID": "posts/solar-searcher/index.html",
    "href": "posts/solar-searcher/index.html",
    "title": "Project Solar-Searcher",
    "section": "",
    "text": "1. Abstract\nAs the severity of climate change continues to increase and the consumption of energy in the US remains massive, a critical focus of today’s power industry is establishing sources of renewable energy to replace less climate-friendly alternatives. Focusing on solar energy systems, we aim to address the problem of locating optimal regions throughout the US to build solar farms for cleaner energy production. Our general approach to this problem is to determine the predicted energy output of installing a solar system at a given coordinate location in the continental US. Leveraging several machine learning techniques to assist our success, we chose to frame this problem as a standard regression task performed by a predictive model. For training and informing the design of our predictive models, we collected, cleaned, and combined location-specific data from several sources on weather, elevation, and solar statistics throughout the continental US. Our modeling approach involves the use of a custom neural network and experimentation with different network layers and nonlinear activation functions. In addition to constructing our custom models, we fit and evaluated a standard linear regression and polynomial regression model from the sklearn library to compare predictive performance across several model architectures (performance assessed via Mean-Squared-Error loss minimization). From our model tuning and comparison procedures, we found that a linear regression model using the ReLU activation function offers the second most accurate solar energy predictions, falling short only to the prebuilt polynomial regression model from sklearn. To visualize and interpret our findings, we developed an interactive map of the continental US, displaying predicted solar energy output (in kWh/kWp \\(-\\) kilowatt-hour per kilowatt-peak) at a resolution of approximately \\(4\\) km \\(^2\\). We intend for our potential solar energy map to stand as a practical tool for guiding future solar power system development and planning.\nThe entirety of our work is accessible in the following public repository: Solar-Searcher\n\n\n2. Introduction\n\n2.1 Motivation\nAs acknowledged above, climate change and global warming continue to rapidly intensify, driving adversity towards billions of people and countless species of wildlife across the planet. In a 2023 report from the World Health Organization, it was conservatively projected that climate change will cause an additional \\(250,000\\) annual deaths by 2030 (World Health Organization 2023). Along similar and possibly more severe lines, the World Wide Fund for Nature reports a “catastrophic \\(73\\%\\) decline in the average size of monitored wildlife populations” over the past 50 years (World Wildlife Fund 2024). Perhaps the most significant threat posed by climate change and global warming is the irreparable and irreversible alteration of the planet’s biosphere. Some experts even fear that the “point of no return” has already been passed. Yet, despite the daunting effects and implications of the current climate circumstances, it remains crucial to direct worldwide attention, technology, and resources towards establishing climate-conscious societies. It is widely recognized that some of the largest contributors to the current climate crisis are carbon-intensive, non-renewable energy production systems. Consequently, large actors of the global power industry, including governmental bodies and officials throughout many nations, are drawing their attention toward the development of planet-supporting energy solutions. Such solutions involve the replacement of preexisting, harmful energy systems with renewable alternatives. Simply put, establishing renewable and regenerative energy sources is an irrefutably necessary step in combating global warming.\nThree of the most common avenues for renewable energy systems are in solar-, hydro-, and wind-powered generators. While these clean-energy sources are proven to be not only highly effective but also regenerative (or supporting of regeneration), they often require a complex set of specific circumstances for construction. Significant renewable energy system limitations include geographical/topological elements and the consistency of weather conditions of a given power production location. To function at peak capacity and comparably to traditional, environmentally abrasive methods, developers aim to install renewable energy systems in regions with the optimal set of conditions. Consequently, the task of identifying these optimal installation locations becomes a primary concern of renewable energy development. As one of the most abundant sources of renewable energy in the continental US, we direct our attention in this study to the development of solar energy systems. Specifically, we intend to address several key concerns of photovoltaic (PV) system implementation including: What factors should solar energy developers take into consideration when initiating installation projects? Based on the relevant factors, which regions are optimal locations for PV system development? At a given location, how can the quality of installing a solar energy system be quantified and compared to other neighboring areas? Driven by these questions, our study offers an approach to solar energy forecasting in the continental US.\n\n\n2.2 Related Work\nSolar energy forecasting has been widely addressed with the use of machine learning tools and techniques. Previous research in this area includes the incorporation of neural network-based models, random forest classifiers, and linear regression techniques \\(-\\) as well as thorough collection and preprocessing procedures of meteorological data \\(-\\) to provide informative insight for predicting solar power production. A 2020 study focusing on solar energy production in Hawaii found a gradient-boosted regression model enhanced with a standard PCA data process to yield strong predictive results for solar power forecasting (Munawar and Wang 2020). Similarly, a 2021 article describes another forecasting framework comparing linear and nonlinear regression models, experimenting with artificial neural network architectures, for predicting solar energy production in Morocco (Jebli et al. 2021). Alternatively, a related 2022 study on solar energy forecasting found compelling results using a modeling approach featuring a machine learning ensemble-to-classifier pipeline operating on standard weather data (Alzubaidi et al. 2022). In our study, we aim to extend the previous research in this field and develop a comprehensible, accessible PV energy prediction framework oriented for the continental US.\n\n\n2.3 Study Overview\nIn the sections below, we present our data collection and cleaning protocols; our model designs, neural network architecture, and model comparisons; our solar forecasting results; and an accompanying discussion addressing our progress and potential future work. Additionally, as a preface to the technical content explored in our study, we offer a brief acknowledgement of the societal implications and impact of our work.\n\n\n\n3. Values Statement\n\n3.1 Affected Parties, Benefits, and Harms\nWe view the potential users of our project to be a fairly wide range, including governmental initiatives, commercial enterprises, and possibly individuals looking to invest in small-scale solar farming. Essentially, any party with potential interest in making use of solar energy has the potential to use our project.\nUnder the assumption that our model will be used to inform solar development, potentially affected parties are the aforementioned end users, who may be benefitted by accurate results informing optimal solar energy development, or harmed if the results are misleading and do not accurately represent the theoretical output. Overall, local communities also have the potential to be impacted, as solar farms do take up a reasonable amount of property, which requires zoning and adequate space (Igini 2023). One other concern is that our application may unfairly harm underrepresented or marginalized groups, as solar panels are expensive to develop, and our model may prioritize more privileged areas. Lastly, it is important to recognize that while our project aims to optimize renewable energy generation, machine learning models require a significant amount of energy to train. While our model specifically only had to undergo training a limited number of times, it is still worth recognizing that in developing cleaner energy, we do have to make use of existing power.\nTo summarize, given the increasing global need for sustainable forms of energy, we believe that our model has the potential to be beneficial to the planet overall if it is used to promote development of solar energy. However, we also recognize the potential harms associated with inaccuracies in the model or over prioritization of certain areas over others.\n\n\n3.2 Personal Investment\nFrom a personal perspective, we all are invested in this project because we see its potential to improve the efficiency of new solar developments, and optimize implementations of renewable energy. We are passionate about increasing the amount of renewable energy used in our ecosystem, and view solar energy as one of the most widely applicable sources. We also believe that leveraging the now widely available data on solar irradiance, weather, and other geological features is a sensible step in the right direction for solar development.\nCommon criticisms of solar energy include its high initial cost, variability with weather, and the requirement of land and materials to make use of this technology (Igini 2023). Given these difficulties, we find that leveraging data to find patterns of efficiency in solar power generation has the potential to make solar development a more viable source of clean, renewable energy. Using weather data as a feature in our model allows us to factor in weather variability by location, and more closely study the correlations between certain weather features and photovoltaic output. By determining the optimal locations for solar farms, we can also minimize the impacts of high initial costs and the land requirements, as fewer farms will be needed to achieve the same end results.\n\n\n3.3 Overall Reflection\nBased on this reflection, our technology has the potential to make the world a more sustainable place, as solar power has the potential to replace harmful fossil fuels in the energy ecosystem. This outcome also has the potential to make the world a more equitable place, as developing solar energy has the potential to make energy more available in the long term. While there are potential harms associated with predicting photovoltaic output by location, we find that on balance, the potential benefits are strong, and the harms can be minimized through thorough evaluation of our models.\n\n\n\n4. Materials and Methods\n\n4.1 Data Collection\nThe data used in our study was gathered from a collection of publicly available sources from national and international organizations. For solar irradiance and photovoltaic information, we collected data from the National Renewable Energy Laboratory (NREL) (National Renewable Energy Laboratory 2025) and the Global Solar Atlas (GSA) from The World Bank (The World Bank Group, ESMAP, and Solargis 2019). For weather and elevation statistics, we pulled data from the Copernicus Climate Change Service (C3S) (Hersbach et al. 2023) and the Google Maps API. Outlined below are more detailed descriptions of the specific data sources used in our study:\n\nPhotovoltaic Output (PVO)\nPhotovoltaic output (measured in kWh/kWp \\(-\\) kilowatt-hour per kilowatt-peak) is a quantitative measurement of the amount of producible energy from a solar/PV power system. Specifically, PVO represents the amount of power generated per unit of a given solar energy installation over the long term. Defined in kWh/kWp, PVO describes the energy output in kilowatt-hours of a single PV unit operating at peak performance (according to standard testing conditions) over a designated, long-term period of time. In general, PVO provides a baseline metric for the energy production capacity of a given solar energy system. We collected PVO data from The World Bank’s Global Solar Atlas (The World Bank Group, ESMAP, and Solargis 2019). The PVO dataset we used contains monthly average “practical potential” PVO values from 1999-2018 for a \\(0.0083^\\circ\\)-latitude by \\(0.0083^\\circ\\)-longitude grid (~\\(1\\) km \\(^2\\)) of the entire US. Each potential PVO value is calculated using a multi-step modeling process combining satellite imagery, meteorological data, and PV system simulations developed by Solargis (Solargis, n.d.). For each month of the year (12 total data subsets), each row represents the potential PVO value at a given latitude-longitude coordinate location of the US. The figure below provides a visual representation of the PVO dataset used in our study.\n\n\n\nFigure 1: Visualizing the potential PVO data for the months of January, April, July, and October.\n\n\nRelating to our primary regression objective, we use PVO as the target variable for our predictive models (see sections below for a more detailed discussion). Considering the crucial role that the collected PVO data plays in our modeling approach, the limitations of this dataset should be addressed. Firstly, this dataset contains monthly average values over a ~20 year span, which may disregard or underrepresent historically significant spikes and drops in PVO. Additionally, the values in this dataset are defined in terms of a single PV unit installed at the optimal panel tilt angle. The size of PV units and the optimal panel tilt can vary considerably from installation to installation. Thus, it should be noted that the values in this dataset may be overgeneralizing the projected PV energy yield for certain US regions. Further, the calculated PVO values in this data depend on numerous other relevant solar radiation and meteorological components, which may result in excessive variable correlations in the context of regression models (see sections below for further discussion on this).\n\n\nIrradiance (GHI)\nSolar irradiance (measured in W/m2) quantifies the instantaneous power of sunlight striking a surface. We used Global Horizontal Irradiance (GHI) – the sum of direct beam and diffuse sky radiation on a level plane – as the primary input for photovoltaic (PV) yield models, since PV output scales roughly linearly with incident irradiance (National Renewable Energy Laboratory (2025)). Our GHI data from the U.S. Department of Energy’s National Renewable Energy Laboratory (NREL), published as the Physical Solar Model version 3 (PSM v3) from 1998-2016. This dataset contains monthly and annual GHI averages covering 0.038-degree latitude by 0.038-degree longitude (roughly 4 km by 4 km). This data was produced by merging satellite cloud-detection with radiative transfer clear-sky (REST2) and cloudy-sky (FARMS) models. Each row corresponds to one grid cell’s location, and reports its twelve monthly mean GHI values. There are some limitations to consider upon using this dataset. Firstly, values are monthly means, meaning they omit potential important peaks or lows. Next, panel tilt – finding the optimal angle to capture GHI – is not captured nor discussed by NREL. Lastly, recent climatic trends are not reflected as this data spans only up until 2016; overall weather patterns and irradiance values may have fluctuated since then.\n\n\nWeather\nWe theorized that the climate of a given area may have a substantial impact on the PVO, as factors like cloud coverage and precipitation would influence the amount of sun available to solar farms. We sourced data from ERA5, the fifth iteration of climate reanalysis data from the European Centre for Medium-Range Weather Forecasts (ECMFW), which combines observations with model data to create a physically accurate and consistent climate dataset (Hersbach et al. 2023). We pulled monthly data from 1999 to 2018 for features cvh (high vegetation cover as a fraction of the latitude-longitude grid), sd (snow depth in m of water), msl (sea level pressure in Pa), tcc (cloud cover as a fraction of the latitude-longitude grid), t2m (2 meter temperature K), u100 (100 meter latitudinal wind component in m/s), v100 (100 meter longitudinal wind component m/s), sf (snowfall in m of water), and tp (total precipitation in m of water). We collected this data for latitude and longitude values in the United States in 0.25 degree increments and took the mean over all months and years. We made heatmaps for vegetation cover and and total precipitation to observe how these variables changed with latitude and lonigitude, which are shown in Figures 3 and 4.\n\n\n\nFigure 3: Total Precipitation Heat Map.\n\n\n\n\n\nFigure 4: Vegetation Cover Heat Map.\n\n\nBecause this dataset is from a reputable source and is fairly comprehensive, we can trust that our data is reliable. However, it is not without its limitations, particularly because the changes in latitude and longitude are quite coarse with each latitude-longitude pair corresponding to a 17.25x17.25 mile grid. While we were able to achieve finer measurements for our other data sources, we needed to make them more coarse in order to be consistent with this data (see Combining the Data). The climate variables we selected were chosen based on what we thought would have the largest impact on PVO, but our decisions were not backed by any quantitative evidence. Should we seek to make a more thorough model, we likely would want to select more climate features (many of which are provided by ERA5) and determine through quantitative analysis (like a correlation plot) which features would be most effective.\n\n\nElevation\nLastly, in our data planning, we theorized that elevation may be correlated with photovoltaic output on the assumption that higher elevation would have more potential to capture the sun’s energy. Given the resolution challenges we were having with our other datasets, and the difficulty of finding matching data for elevation, we elected to use the Google Maps Elevation API to retrieve elevation data. With this approach, we can simply make API requests for latitude longitude pairs after all of the other data has been combined. The API accepts latitude and longitude, and returns the elevation of that location, as well as the resolution of the measurement, in meters (Google Developers 2024). The measurement is relative to sea level, so elevation values can be positive or negative. Google acquired this data using NASA’s shuttle radar program (Google Developers 2024)\nThe main limitation affecting this dataset is the variable resolution of the measurements. Areas with higher resolution may more accurately capture terrain features such as small hills or cliffs, while areas with lower resolution may be slightly misrepresented, or smoothed over. As a result, if our model finds that elevation is highly correlated with photovoltaic output, the resolution could be a source of bias in our results. A more practical limitation are the rate limits: we can only make 512 elevation requests at a time, and only a certain number of requests per day. Luckily, this was only a minor issue for our data collection, simply requiring us to write a batch request function to perform 512 requests at a time until we had the complete dataset.\nThe figure below shows the correlation between elevation and photovoltaic output in our final dataset, on a small representative sample.\n\n\n\nFigure 5: Elevation vs Photovoltaic Output.\n\n\nAs the figure shows, there is some positive correlation between elevation and photovoltaic output, but it is a noisy relationship, and much of the elevation data is clustered in the zero to five hundred meter range. As a result, elevation was not the most strongly correlated feature in our dataset, but the outlying areas of high elevation showed strong photovoltaic output.\n\n\n\n4.2 Dataset Compilation & Construction\nWith all of these data sources in place, we began our process of combining the datasets into a single csv file by reading our .tif image files. We wrote conversion functions which would read the data from the .tif files, construct a dataframe containing the relevant rows and columns, then export that dataframe to a csv. At that point, we had constructed the relevant csvs, and then needed to deal with the issue of resolution.\nUnfortunately, because our irradiance data was subject to a lower resolution than our photovoltaic output data, we were forced to downsample the resolution to two decimal places. To accomplish this, we simply created a temporary rounded column, then used an inner join on two dataframes using that rounded column, before dropping the mismatched columns and renaming the rounded column accordingly. This allowed us to combine our two key solar datasets, and though we did lose resolution, we were able to keep all of the relevant data points. However, merging this dataset with the weather dataset was a more complicated issue. If we used the same downsampling technique, we would lose a fair number of our data points, due to a mismatch in the latitude and longitude ranges between the solar and weather datasets. Because these datasets were collected separately from different sources, there were different ranges of latitude and longitude, though there was a great deal of overlap since both datasets pertain to the United States, of course. To account for this difficulty, we used a cKDtree, which allows a nearest-neighbor approach to merging the dataset. Essentially, this structure indexes the latitudes and longitudes in the solar dataset, then queries the weather dataset to find the nearest neighboring points, merging values within a certain range. By taking this approach, we were able to maintain over 400000 data points in our final csv, which is a dataset size that we were very happy to achieve.\nThe last step after this combination was to simply use the batch request function to query the Google Maps API for the relevant latitudes and longitudes, then export the finalized dataframe to a csv for ease of access between notebooks. With the full dataset in place, we constructed a correlation matrix for our features, which shows the strength and positive or negative value of correlation between each feature. The correlation matrix is shown below.\n\n\n\nFigure 6: Correlation Matrix.\n\n\nWhile there are many interesting trends to observe in this correlation matrix, we were mostly concerned with the correlations to photovoltaic output (pvo). The strongest positive correlation was with solar irradiance, which follows our expectations, given that irradiance refers to the strength of the solar beams in a given location. Interestingly, the strongest negative correlation with photovoltaic output was total cloud cover, having even more of an impact than irradiance. This exemplifies one of the primary challenges with solar energy, in that cloud cover blocking the sun can dramatically harm energy output. Other key correlations include elevation, total precipitation, vegetation cover height, and sea level pressure.\n\n\n4.3 Model Design\nWe constructed a sequential neural network using PyTorch to try and predict the PVO based on our collected data. We used three linear layers (decreasing by 2-4 neurons with each layer) with a non-linear activation function between each layer. We experimented with a model where we used ReLU functions and one where we used sigmoid activation functions. We evaluated the loss at each epoch and used the stochastic gradient descent optimizer with a learning rate of 0.01. We determined through trial and error that stochastic gradient descent was a more effective optimizer for this problem than Adam and that a learning rate of 0.01 was sufficient to accurately train the model without overfitting. The code for our LinearModel class is shown below. For the models that we used the sigmoid activation function, we replaced all ReLU() calls with a Sigmoid() call.\n    class LinearModel(nn.Module):\n        \n        def __init__(self, all_feats = False): # \"all_feats\" arg. passed from above\n            \n            # Initialize nn.Module object\n            super().__init__()\n\n            # Matrix alignment depending on if all features are used\n            if all_feats:\n                init_feats = 13\n            else:\n                init_feats = 12\n\n            # Basic linear model pipeline\n            self.pipeline = nn.Sequential(\n                nn.Linear(init_feats, 10),\n                ReLU(),\n                nn.Linear(10, 6),\n                ReLU(),\n                nn.Linear(6,2),\n                ReLU(),\n                nn.Linear(2,1)\n            )\n\n\n4.4 Model Training & Evaluation\nBefore feeding our data to the neural network, we rescaled it using a standard scaling and separated the PVO column into a separate target column. Using SciKit-Learn’s train_test_split() function, we divided the data into a training and testing dataset where 30% of the data was reserved for testing. We batched each dataset using the torch dataloader torch.utils.data.DataLoader() into batches of size 32.\nIn our preliminary data analysis, we found that irradiance had a strong correlation with PVO. To determine how impactful this would be on the accuracy of our model, we trained models both with and without the irradiance feature and compared their testing losses. The init_feats parameter in our model archietecture accounts for this, allowing the model to accept a dataset that may or may not contain the irradiance feature. We thus trained four models: Irradiance included with ReLU activation, Irradiance included with sigmoid activation, Irradiance excluded with ReLU activation, Irradiance excluded with sigmoid activation. We trained each for 100 epochs on the GPUs provided by the Middlebury College Physics Department research computers. The entire code for our model can be found here.\nTo compare our approach to existing models, we trained a linear regression model and polynomial regression model from SciKit-Learn on our data as well. We evaluated the testing loss using mean squared error, allowing us to directly compare the sklearn models to our own. We comapre each of these approaches in the results section.\n\n\n\n5. Results\nUsing a Mean Squared Error loss function, we tracked the progress of each of our models and then used the testing loss after the final epoch to evaluate how well our model performed when predicting PVO. We show the loss plots for our four models below.\n\n\n\nFigure 7: Training and Testing Loss.\n\n\nBased on these plots, it appears that the sigmoid mdoels had a much smoother improvement in loss, but ultimately converged to a slightly higher loss value than the ReLU models. We observe the best testing loss with the Irradiance ReLU model with a MSE of \\(0.0058\\) and the worst testing loss with the No Irradiance Sigmoid model with a MSE of \\(0.0079\\). The table below shows the testing loss for each model.\n\n\n\nModel\nMSE\n\n\n\n\nIrradiance ReLU\n\\(0.0058\\)\n\n\nIrradiance Sigmoid\n\\(0.0065\\)\n\n\nNo Irradiance ReLU\n\\(0.0071\\)\n\n\nNo Irradiance Sigmoid\n\\(0.0079\\)\n\n\nSklearn Linear Model\n\\(0.0104\\)\n\n\nSklearn Third Order Polynomial Model\n\\(0.0050\\)\n\n\n\nEach of our neural network models outperformed the standard SKlearn linear model, meaning the increase in parameters due to deep learning allowed for an improvement in our predictive abilities. However, our best model still underperformed the SKlearn polynomial regression model, meaning there is room for improvement in our deep learning models. We could potentially achieve this by training our models for more epochs, adjusting the parameters in each layer of our model, or implementing non-linear archietecture such as a convolutional neural netowrk to better train to the data. Despite the varying efficacy of these models, all are able to achieve very low loss values relative to the actual PVO values.\nUtilizing the Irradiance ReLU model, we created a map that comprehensively showcases our end results. The map visualizes predicted photovoltaic (PV) output across the United States, displaying both actual and predicted values for various locations. The interactive visualization uses color intensity to represent the predicted solar energy production potential, allowing for comparison between actual and model-predicted values throughout different geographic regions. The map is shown below.\n\n\n\n\n6. Discussion\nWe set out to answer a simple but high-stakes question: Can we predict the potential solar energy output across the United States? By fusing publicly available irradiance, climate, elevation, and historical PV-output data onto a unified 4-km grid, we trained and benchmarked several regression models, ultimately creating a model architecture that was highly accurate nationwide. The predictions are shown through an interactive map that anyone can easily explore. In short, our project turns scattered open data into an actionable siting tool, closing the gap between technical resource assessments and real-world solar deployment decisions.\nIn the end, we achieved the core objectives that we set at the start. Our minimum-viable goal was to turn the open data we collected into a working model that reliably predicts photovoltaic output across the U.S. and present those predictions in an easily digestible format. Despite initially exploring wind and hydro, we discovered that collecting, cleaning, and combining the solar-specific data alone was a substantial undertaking, thus we decided to only focus on solar, while still expressing interest in wind and hydro interest in potential future work (which will be discussed in more detail below).\nCompared with earlier regional studies, our models are competitive – even after scaling the problem to the entire continental United States. Munawar & Wang (Munawar and Wang 2020) reported a 4% normalized RMSE (nRMSE) for short-term PV forecasts in Hawai‘i using XGBoost + PCA, and Jebli et al.(Jebli et al. 2021) achieved a 6% nRMSE with an ANN in semi-arid Morocco. By contrast, our best nationwide model – the Irradiance-ReLU network – delivers a 2%* nRMSE and cuts mean-squared error by 44% relative to a plain sklearn linear regression baseline (0.0058 vs 0.0104). That said, sklearn’s third-order polynomial regressor edges out the neural network (MSE = 0.0050), reminding us that good feature engineering can sometimes outperform added model depth. In short, we match or surpass the accuracy of the regional studies while covering a far larger, more diverse geography, and we do so with models that remain tractable for potential real-world use.\n**Normalized RMSE was computed by first converting MSE to RMSE, then normalizing by the average PVO (about 4 kWh/kWp).*\n\\[\\text{nRMSE}=\\frac{\\sqrt{\\text{MSE}}}{\\bar{\\text{PVO}}} =\\frac{\\sqrt{0.0058}}{4.0\\ \\text{kWh/kWp}^{-1}}\\approx0.019\\;(\\text{≈ 2 \\%})\\]\nWith additional time, more data, and stronger compute, we would push the project in 3 potential further directions. First, we’d incorporate other renewables – wind and hydro – by sourcing high-resolution wind‐resource and streamflow archives and integrating them into our 4 km grid. This would give much more context for potential renewable energy development, giving a more comprehensive view of which renewable source might be best suited for a certain location. Second, we’d move from a yearly climatology average to more time-specific data, potentially refining our data to utilize daily or hourly measurements. This would allow our models to capture swings in our features, shedding light on potentially crucial intricacies of solar farm developments, such as panel tilt optimization, shading and soil losses, and overall more time-specific weather fluctuations. Third, we’d overlay economic and land-use constraints – site cost, permitting zones, grid capacity – and compute the levelized cost of energy (LCOE) rather than just kWh/kWp. By combining predicted energy yields with spatial cost surfaces and zoning maps, we could pinpoint sites that minimize LCOE or maximize return on investment. On the modeling side, more compute would let us train deeper neural architectures, hypertune our loss functions and activations, and deploy active-learning loops that continually refine forecasts as new utility-scale production data arrive, allowing us to provide more real time predictions. Together, these enhancements would evolve our prototype into a production-grade platform for optimally siting clean energy systems at continental scale.\n\n\n7. Group Contribution Statement\nOverall, we worked together on the writing of this blog post (except, of course, for the personal reflection).\n\n7.1 Omar Armbruster\nOmar worked on collecting and compiling the weather data used in our dataset. Using the weather data, he was able to create some preliminary heatmaps to visualize the features in different regions of the U.S. He also built the foundation of the neural network (including features like model saving and loading), built the data processing pipeline, and wrote the scripts needed to train the model on the physics department computers remotely. He led the writing of the methods section and the weather data section of this blog post and wrote the loss discussion in the results section.\n\n\n7.2 Andrew Dean\nAndrew collected irradiance data from NREL and developed initial visualizations to explore spatial patterns across the U.S. He then collaborated with Col to merge this irradiance data with Photovoltaic Output (PVO) metrics, calculating annual averages for each feature at every coordinate point. To synthesize the results, Andrew produced an interactive U.S. map using plotly, standardizing the input features and applying Omar’s pre-trained neural network to generate predicted PV values. Each point on the map displays both actual and predicted PV outputs, enabling clear visual comparison. He led the writing of the irradiance and discussion sections of this blog post, and contributed to the results section.\n\n\n7.3 Col McDermott\nContributing to the source code, Col collected and cleaned the potential photovoltaic output data from the GSA. He then collaborated with Andrew to merge the GHI and PVO data into a single dataset of solar information, collapsing the combined data into the overall annual average (across all \\(12\\) months) of each feature for every coordinate data point. Additionally, Col worked alongside Andrew and Noah to combine the solar and weather/elevation data into a single curated dataset. Constructing the full dataset involved incorporating a cKDTree from the scipy library to merge datasets with different latitude-longitude resolutions while maintaining a sufficient number of data points and ensuring the absence of any null values. With the full dataset established, Col created some preliminary visualizations to observe potential trends and relationships between various features. Col also assisted with debugging the model design and improving the training scripts. For the project report, Col wrote the abstract and introduction sections (involving research on present-day climate events and some related work in the field) as well as the brief discussion on PVO data collection. In addition to contributing these components, Col worked on organizing and tidying the layout of the full written report.\n\n\n7.4 Noah Price\nNoah began the project working on collecting elevation data for our dataset, which involved establishing a connection to the Google Maps Elevation API using a private key system. He then worked on combining the data from each of our data sources, which involved rescaling the coarseness of the latitude and longitude for some features so that they could be joined with the climate data. With the data in place, he debugged and made improvements to the neural network architecture and infrastructure to optimize the model’s performance and created data visualizations for preliminary data analysis. He led the writing of the values section, combining the data section, and elevation data section of this blog post.\n\n\n\n8. Personal Reflection\nThis project was a great way to practice the entire data processing and model-building pipeline, which will be useful for proceeding with future machine learning projects. I thought it was very interesting to apply the methods we have learned in class to empirical data and use our models to make predictions that could give us meaningful insight into real and important issues. I really enjoyed getting to research and work with data pertaining to climate and renewable energy as this is not a topic I typically do much with in my daily studies. I thought it was both interesting and challenging to work with geospatial data, which required us to format our data so we could work with the latitude and longitude values corresponding to the U.S., which was especially important when creating our heatmaps. I also learned a lot from working on this project in a group, as we had to coordinate our efforts and delegate the responsibilites so that we could be as efficient as possible. This sometimes took some extra work to make sure we could fit all of our individual pieces together (such as when we were combining our data), but I believe this ultimately allowed us to produce a higher quality project.\nI am somewhat satisfied with what we achieved. While we did fall short of some of our goals (performing a cost analysis and applying other forms of renewable energy), we were still able to build a strong model that was able to predict solar output on par with similar models from SKlearn and were able to produce engaging and interactive heatmaps with our predictions. Our shortcomings with our deliverables were likely due to the challenging data processing steps. While we had access to a lot of data, it was challenging to find the specific data we needed and it was more challenging to combine the data that we did have. In light of this, I am very pleased with what we were able to accomplish and I feel that we were able to construct a consistent and meaningful dataset. While our neural network architecture was fairly straightforward, I enjoyed getting to examine questions like “Does Irradiance heavily impact the model?” and “How does activation function impact the model?” as it allowed us to dive deeper into the inner workings of our model and examine how different parameters influenced predictive outcomes. We were not originally planning on comparing our models to the standard SKlearn model, but I feel that this also added an extra layer of nuance and investigation to our project. We were able to give some thought into how we could adapt our data to be used in a CNN (potentially by creating multi-channel images where each variable corresponds to a channel and then breaking the entire latitude-longitude set into smaller squares to create testing and training data), but unfortunately did not have time to actually implement it. Should we choose to ever pick up on this project, I think it would be interesting to explore this, as a CNN could potentially lead to increases in accuracy that we could use to outperform the SKlearn polynomial regression model.\nOverall, I thought we had a fairly successful project and I enjoyed getting to explore data in a field of study that I typically don’t interact with. I think the process of working in a group on data projects as well as practicing working with and processing data from a variety of sources will be very valuable in my future endeavors as a data scientist. Throughout this process we had to be creative and persistant to find the optimal way of sourcing and combining our data, which I believe will come in handy when dealing with difficult data in the future. I also think the process of building neural networks from scratch and analyzing the strengths and weaknesses of different models will be very important for just about any machine learning project I choose to undertake in the future.\n\n\n9. References\n\n\n\n\nAlzubaidi, Laith, Jinglan Zhang, Ali J. Humaidi, Abdulmuttalib Al-Dujaili, Yuxuan Duan, Omar Al-Shamma, Juan Santamaría, Mohammed A. Fadhel, Mohammed Al-Amidie, and Laith Farhan. 2022. “Review of Deep Learning: Concepts, CNN Architectures, Challenges, Applications, Future Directions.” Journal of Big Data 9 (1): 1–74. https://doi.org/10.1155/2022/7797488.\n\n\nGoogle Developers. 2024. “Elevation API Overview.” https://developers.google.com/maps/documentation/elevation/overview.\n\n\nHersbach, Hans, B. Bell, P. Berrisford, G. Biavati, A. Horányi, J. Muñoz Sabater, J. Nicolas, et al. 2023. “ERA5 Monthly Averaged Data on Single Levels from 1940 to Present.” Copernicus Climate Change Service (C3S) Climate Data Store (CDS). https://doi.org/10.24381/cds.f17050d7.\n\n\nIgini, Martina. 2023. “What Are the Advantages and Disadvantages of Solar Energy?” https://earth.org/what-are-the-advantages-and-disadvantages-of-solar-energy/.\n\n\nJebli, Imane, Fatima-Zahra Belouadha, Mohammed Issam Kabbaj, and Amine Tilioua. 2021. “Prediction of Solar Energy Guided by Pearson Correlation Using Machine Learning.” Energy 224: 120109. https://doi.org/https://doi.org/10.1016/j.energy.2021.120109.\n\n\nMunawar, Usman, and Zhifang Wang. 2020. “A Framework of Using Machine Learning Approaches for Short-Term Solar Power Forecasting.” Journal of Electrical Engineering & Technology 15 (2): 561–69. https://doi.org/10.1007/s42835-020-00346-4.\n\n\nNational Renewable Energy Laboratory. 2025. “Solar Resource Maps and Data Portal.” https://www.nrel.gov/gis/solar-resource-maps.\n\n\nSolargis. n.d. “Solar Radiation.” https://kb.solargis.com/docs/solar-radiation.\n\n\nThe World Bank Group, ESMAP, and Solargis. 2019. “Global Solar Atlas 2.0: LTAy_AvgDailyTotals (GeoTIFF) – USA Solar Resource Data.” https://globalsolaratlas.info/download/usa.\n\n\nWorld Health Organization. 2023. “Climate Change and Health.” https://www.who.int/news-room/fact-sheets/detail/climate-change-and-health.\n\n\nWorld Wildlife Fund. 2024. “Catastrophic 73% Decline in the Average Size of Global Wildlife Populations in Just 50 Years Reveals a System in Peril.” https://www.worldwildlife.org/press-releases/catastrophic-73-decline-in-the-average-size-of-global-wildlife-populations-in-just-50-years-reveals-a-system-in-peril."
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "Abstract\nMachine learning methods provide the opportunity to extract features from a dataset that would otherwise be hidden. In this blog post, we examine Linear Regression and Random Forest Classifier models and train them on the Palmer Penguins dataset to show that we can reliably identify which of three species a penguin belongs to based on three of their characteristics. Using an exhaustive search, we identify that using sex, culmen length, and clumen depth, we can predict a penguin’s species with at least a 98% accuracy.\n\n\nLoading the data\nIn order to begin our deep dive into the Palmer Penguins dataset, we must first load the data into our python notebook, which we achieve with the read_csv function from the pandas library. As we will see momentarily, the species labels contain the english and latin names for the penguins. For the sake of simplicity, we extract the first word in each species label, leaving us with three simple species labels: Chinstrap, Gentoo, and Adelie.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\nIt’s always a good idea to get an idea of what the data looks like before diving into the analysis. Based on this snapshot of the data, we can start considering which qualitative and quantitative variables may be useful to us when fitting our models. Features like “studyName”, “Sample Number”, and “Individual ID” are likely not useful features (since they have nothing to do with the natural features of a penguin), but it may be worth looking at the other qualitative features like “Region”, “Island”, “Clutch Completion”, or “Sex”. Quantitative features like “Culmen Length”, “Culmen Depth”, and “Flipper Length” will also likely be useful.\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\nExploratory data analysis\nBefore we jump straight into model fitting, it’s a good idea to stop for a moment (Freeze if you will) and get an idea of which features will be most useful to our models. There are a variety of outcomes (an ocean of possibility perhaps?) that can arise based on which combinations of features we choose, so by examining the relationships between features of the data, we can get an idea for which features will produce the highest predictive accuracy. Using the matplotlib and seaborn packages, we can visualize our data, which may provide us with some of these insights.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\nOur scatterplot examines two qualitative variables: “Delta 13 C” and “Delta 15 N”, which are isotopes that were found in the blood of the Palmer Penguins (Don’t ever say that data science is easy; those penguins put their blood sweat and tears into this dataset!). Based on the scatterplot, the two isotopes do provide some amount of clustering between each species. However, the Adelie region does appear to overlap quite a bit with the other two regions, meaning that we would likely not want to use these features without a third feature to distinguish between penguins in the overlapping regions.\nThe stack plot examines the quantity of penguins for each species on each of the three islands included in this study. It appears that Gentoo penguins only appear on Biscoe Island and Chinstrap penguins only appear on Dream Island, while Adelie penguins appear on all three islands. Based on this information, if we have the name of the island, we can determine if a penguin is not a Gentoo or not a Chinstrap. However, we cannot definitively determine which species the penguin is (unless it’s on Torgersen island where it’s likely an Adelie) without additional features.\n\nfig, ax = plt.subplots(1, 2, figsize = (7, 4))\n\np1 = sns.scatterplot(train, x = 'Delta 15 N (o/oo)', y = 'Delta 13 C (o/oo)', hue = 'Species', ax = ax[0])\np2 = so.Plot(train, x = 'Island', color = 'Species').add(so.Bar(), so.Count(), so.Stack())\np2.on(ax[1]).show()\n\n\n\n\n\n\n\n\nWe can also take a closer look at the quantitative variables by looking at the mean and standard deviation values of a variety of features for each species. If the mean is not within a standard deviation of the means of the other two species, then it is likely that that feature could be a distinguinshable trait. With all four of these features, it appears that at least two species are always within a standard deviation of one another, meaning none of these traits can be used alone to classify the penguins.\nBased on these obervations, it is likely that we will need to use some combination of features to fit an accurate model, as it appears that there is no feature or pair of features that can singlehandedly predict the species.\n\ntrain.groupby(\"Species\").aggregate({'Flipper Length (mm)': ['mean', 'std'],\n                                    'Culmen Length (mm)': ['mean', 'std'],\n                                    'Culmen Depth (mm)': ['mean', 'std'],\n                                    'Body Mass (g)' : ['mean', 'std']})\n\n\n\n\n\n\n\n\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\nBody Mass (g)\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n190.084034\n6.689962\n38.970588\n2.640729\n18.409244\n1.217039\n3718.487395\n462.657007\n\n\nChinstrap\n196.000000\n7.423419\n48.826316\n3.450234\n18.366667\n1.138033\n3743.421053\n407.423479\n\n\nGentoo\n216.752577\n5.933715\n47.073196\n2.737415\n14.914433\n1.003431\n5039.948454\n498.861461\n\n\n\n\n\n\n\n\n\nData Preparation\nOur data requires a bit of preparation before it can be fed into our machine learning models. Our models only process quantitative variables, so we need to first represent our qualitative variables as numbers. Using the LabelEncoder, we convert the species labels to ternary form such that each species of penguin is represented by a \\(0\\), \\(1\\), or \\(2\\). We drop the qualitative features that are dependent on the study, rather than an observation made of a penguin, which allows us to isolate the features that could be used to identify a penguin even if it came from a completely different study. Using get_dummies(), we convert the remaining qualitative variables to a set of dummy columns, which express each qualitative outcome as booleans. Our prepare_data function returns X_train: the data as a set of binaries and quantiative variables, and y_train: the ternary set of labels corresponding to each penguin in the study, which we can plug straight into our machine learning models!\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nFrom a brief examination of X_train, we can see that we successfully isolated the quantitative variables and converted the meaningful qualitative variables to a set of booleans!\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nModel and Feature Selection\nOur goal for our model is to select two quantitative features and a qualitative features that we can use to optimize the classification accuracy. For this analysis, we choose to do an exhaustive search, which tests the predictive accuracy of all combinations of one qualitative and two quantitative features and selects whichever combination scores the highest. Because we divided our qualitative features into dummy columns earlier, selecting qualitative features will involve using multiple qualitative columns (ex: using the “Island” feature will use three columns).\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\ncol_combs = []\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols \n    # print(cols)\n    col_combs.append(cols) \n\nWe first choose to examine the Logistic Regression model, which we use to determine which combinations of features score the highest. Iterating through the combinations of columns we created above, we can fit every possible combination of features to a Logistic Regression model and update our best score and best columns each time we find a model that works better than the previous best.\n\n%%capture\nfrom sklearn.linear_model import LogisticRegression\nbest_score = 0\nbest_cols = []\nbest_model = LogisticRegression()\nfor col in col_combs:\n    LR = LogisticRegression()\n    LR.fit(X_train[col], y_train)\n    new_score = LR.score(X_train[col], y_train)\n    if new_score &gt; best_score:\n        best_score = new_score\n        best_cols = col\n        best_model = LR\n\nOur exhaustive search method returns that the three best features for classification are “Culmen Length”, “Culmen Depth”, and “Sex”. When using these three features on our training dataset, we achieve a classification accuracy of \\(99.6\\)%!.\n\nprint(best_score, best_cols)\n\n0.99609375 ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\n\n\nThe goal with building a machine learning classifier is to be able to accurately classify data that we don’t have the labels for. While a \\(99.6\\)% accuracy is certaintly exciting, it is not as impressive when we consider that the model has already seen the data that it is making predictions for. To test how applicable the model will be to new data, we can employ cross-validation, which trains the model and scores its predictions based on different subsets of the training data. When we calculate the mean of each subset’s score, we find that the model has an average score of \\(98.4\\)%, meaning that the model will likely perform well on new data and is not subject to overfitting! We can also use this cross-validation score as a comparison to other machine learning models to determine which will be most effective.\n\n%%capture\nfrom sklearn.model_selection import cross_val_score\ncv_scores_LR = cross_val_score(LR, X_train[best_cols], y_train, cv = 5)\n\n\ncv_scores_LR.mean()\n\n0.9844645550527904\n\n\nFor the sake of fairness, let’s take a look at a second machine learning model to see if we can improve upon the results of our Logistic Regression model. We can fit a Random Forest Classifier model just like we do with Logisitic Regression and we find that it achieves \\(100\\)% accuracy on the training data set!\n\nfrom sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier()\nRF.fit(X_train[best_cols], y_train)\nRF.score(X_train[best_cols], y_train)\n\n1.0\n\n\n\ncv_scores_RF = cross_val_score(RF, X_train[best_cols], y_train, cv = 5)\ncv_scores_RF.mean()\n\n0.9765460030165913\n\n\nNow comes the moment of truth: seeing how well our models work on a new set of data. We import and prepare the data the same way we did with our training data, which we then pass into each model’s scoring method. We need to be careful not to refit the models on this data, as the whole idea is to test how well our previous fitting works on new data. As we see below, the Logistic Regression and Random Forest Classifier models score \\(100\\)% and \\(98.5\\)% accuracy, respectively. Success!\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\nX_test, y_test = prepare_data(test)\ntest_LR = best_model.score(X_test[best_cols], y_test)\ntest_RF = RF.score(X_test[best_cols], y_test)\nprint(test_LR, test_RF)\n\n1.0 0.9852941176470589\n\n\n\n\nVisualizing the Model\nUsing a decision regions plot, we can visualize how our model assigns its labels. We can plot the two quantitative variables against one another, which creates a coordinate system with each point on the graph representing an individual penguin. Our models seek to define boundaries between each cluster of points, which in theory should divide the data into groups based on species. Because of the slightly superior performance of the Logisitic Regression model, we will examine its decision regions for both its training and testing data.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nBelow are the decision regions for the training dataset, divided by “Sex”, our selected qualitative variable. As we can see, the Logistic Regression model divides the coordinate space into three regions, each which correspond to a penguin species. When provided with new data, the model identifies where in the coordinate space that the new data point falls, and assigns a label based on whichever region it falls in. It appears that there are a few points that cross over to a different species’ region, which would account for the less than perfect accuracy we observed earlier. While our first instinct may be to try and fix the model so that it achieves perfect accuracy, this is not necessarily the goal, as redrawing our lines to accomodate for outliers will likely cause overfitting that will not translate well to new data. As such, these regions indicate a very strong classifier!\n\nplot_regions(best_model, X_train[best_cols], y_train)\n\n\n\n\n\n\n\n\nBelow shows the same decision regions applied to the testing data, and as we would expect, the new data aligns very well with our trained regions. Success Again!\n\nplot_regions(best_model, X_test[best_cols], y_test)\n\n\n\n\n\n\n\n\nAs a final check, it helps to generate a confusion matrix, which compares the real labels to the predicted labels to show where the classifier is making mistakes. For example, we may notice that the model tends to sometimes classify Adelie penguins as Gentoo penguins but never Gentoo penguins as Adelie penguins. This would perhaps help us identify similar features between the two or would maybe indicate that our model needs more training to be able to distinguish between the two. In our case, our model is fairly accurate, so it is a little tricky to make any strong conclusions about the errors in the model. Based on the confusion matrix for the training data below, it appears that one of the Chinstrap penguins was classified as a Gentoo penguin.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_train_pred  = best_model.predict(X_train[best_cols])\nC = confusion_matrix(y_train, y_train_pred)\nC\n\narray([[108,   0,   0],\n       [  1,  55,   0],\n       [  0,   0,  92]], dtype=int64)\n\n\n\n\nDiscussion\nUsing our machine learning models, we show that we can learn trends in the data and use them to classify unlabeled data that the models haven’t seen before. This can be a powerful tool as enabling machines to make educated decisions can allow for huge bounds in automation as well as allow for the indentification of trends that we would not be able to identify on our own. Based on the results above, it is possible to achieve optimal and effective clustering with only a few features, demonstrating how power and potential of these models."
  },
  {
    "objectID": "posts/double-descent/index.html",
    "href": "posts/double-descent/index.html",
    "title": "Observing Double Descent",
    "section": "",
    "text": "Abstract\nAfter thorough investigation, scientists have concluded that repeatedly banging your head against a wall is not conducive to positive learning outcomes. Some studies have even gone on to conclude that such practices actually inhibit learning and development. However, some research has indicated that we may not be able to make such conclusions when it comes to teaching machines. In fact, such practices may allow us to achieve levels of accuracy previously thought to be impossible. It has been observed that as the number of features increases relative to the number of observations, the model will become more overfit and will thus have a higher testing loss. This testing loss will asymptotically approach the point where the number of features and observations are equivalent. Very counterintuitively, if we continue to make the loss worse by adding more features, we will ultimately reach a point where the loss begins to decrease again, plateauing at a testing loss lower than was previously observed. This phenomenon known as double descent is an extremely desirable outcome for training reliable machine learning models and is the reason why deep learning models are able to achieve such high levels of accuracy with so many parameters. This blog post examines the double descent phenomenon by implementing and overfitting a linear regression model. We apply this method to a set of corrupted images and observe this change in the loss as we vary the number of features used to fit the model.\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nIn order to test the effects of varying parameter sizes, we need to first define the RandomFeatures class, which selects \\(n\\) random features and passes them through either a sigmoid activation function given by \\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\] or a square activation function given by \\[square(x) = x^2.\\] Using this approach, we can choose a random set of features (to avoid having our analysis be biased towards a specific singular feature) that are transformed with one of the two defined activation functions.\n\ndef sig(x): \n    return 1/(1+torch.exp(-x))\n\ndef square(x): \n    return x**2\n\nclass RandomFeatures:\n    \"\"\"\n    Random sigmoidal feature map. This feature map must be \"fit\" before use, like this: \n\n    phi = RandomFeatures(n_features = 10)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n\n    model.fit(X_train_phi, y_train)\n    model.score(X_test_phi, y_test)\n\n    It is important to fit the feature map once on the training set and zero times on the test set. \n    \"\"\"\n\n    def __init__(self, n_features, activation = sig):\n        self.n_features = n_features\n        self.u = None\n        self.b = None\n        self.activation = activation\n\n    def fit(self, X):\n        self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n        self.b = torch.rand((self.n_features), dtype = torch.float64) \n\n    def transform(self, X):\n        return self.activation(X @ self.u + self.b)\n\n\n\nImplementing Linear Regression\nThe standard formula for the optimal weight vector in least squares is given by \\[\\hat{\\textbf{w}} = argmin_{\\textbf{w}}||\\textbf{Xw} - \\textbf{y}||^2,\\] which has the closed-form solution \\[\\hat{\\textbf{w}} = (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}\\] only when the number of data observations \\(n\\) is larger than the number of features \\(p\\). This is because if \\(p &gt; n\\), the invertible matrix theorem will be violated, making it impossible to find the inverse of \\((\\textbf{X}^T\\textbf{X})\\). When \\(p &gt; n\\), there are more columns of \\(\\textbf{X}\\) than rows, meaning that it is impossible for each of the columns to be linearly independent, which is a requirement by the invertible matrix theorem for a matrix to be invertible.\nAs such, this solution will not hold for our experiments, meaning we need to alter our definition for the weight vector in our implementation of linear regression. We do this using the formula \\[\\hat{\\textbf{w}} = \\textbf{X}^+\\textbf{y}\\] where \\(\\textbf{X}^+\\) is defined as the Moore-Penrose pseudoinverse of \\(\\textbf{X}\\). We accomplish this with the function torch.linalg.pinv() and use this to calculate our optimal weight vector in our MyLinearRegression implementation.\n\n%load_ext autoreload\n%autoreload 2\nfrom linear import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nSimple Data\nWe first examine the efficacy of our algorithm on a simple set of data, scattered randomly in a parabolic shape. Using our linear regression model, we expect to be able to fit these data quite accurately. We can first start by plotting the data to be fit, which is roughly parabolic as expected.\n\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\nplt.scatter(X, y, color='darkgrey', label='Data')\n\n\n\n\n\n\n\n\nWe can then define our training dataset using the RandomFeatures class as discussed above. We choose \\(10\\) features at random to feed to our model and transform them with the sigmoid activation function. We then define our linear regression model and our optimizer and fit the model on our feature data. Unlike previous algorithms, we do not need to solve the weights iteratively, as we can calculate the weight vector directly using the Moore-Penrose approach above.\n\nphi = RandomFeatures(n_features = 10)\nphi.fit(X)\nX_train_features = phi.transform(X)\n\nLR = MyLinearRegression()\nopt = OverParameterizedLinearRegressionOptimizer(LR)\nopt.fit(X_train_features, y)\n# now LR.w has the optimal weight vector \n\nWe can then define our predictions based on the calculated weights and plot them against the original data. The model appears to be a very good fit of the data. While the model cannot predict the values perfectly accurate, it appears that our predictions follow the same trend as the data and will make the correct predictions on average.\n\npred = LR.predict(X_train_features)\nplt.scatter(X, y, color='darkgrey', label='Data')\nplt.plot(X, pred, color = 'red', label = 'Prediction')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nCorrupted Data and Overfitting\nNow that we have confirmed our model is able to effectively fit a given dataset, we can examine the model’s performance on more difficult problems. For this experiment, we will examine corrupted images where sections of the image have been removed. The goal of our model will be to fill in this missing sections as accurately as possible. The original, uncorrupted image can be seen below.\n\nfrom sklearn.datasets import load_sample_images\nfrom sklearn.model_selection import train_test_split\nfrom scipy.ndimage import zoom\n\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X,.2) #decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower, cmap = 'gray_r')\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\nWe then define the corruption function, which will allow us to randomly remove sections of the image for our model to predict.\n\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\nAn example of the corrupted image can be seen below. Each gray square represents a missing section of the data for our linear regression model to predict.\n\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1, cmap = 'gray_r')\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\nUsing our method for corrupting images, we can create training and testing datasets. We specify \\(n\\_samples = 200\\), meaning each of our datasets will have \\(100\\) images in them. We then apply our corrupted_image() function to \\(200\\) versions of our image and split them randomly into training and testing data.\n\nn_samples = 200\n\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = torch.float64)\ny = torch.zeros(n_samples, dtype = torch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\nX = X.reshape(n_samples, -1)\n# X.reshape(n_samples, -1).size()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\nNow that we have our training and testing datasets, we can examine the performance of our linear regression model as the number of features varies. We fit and evaluate the model for \\(0-400\\) features and store the training and testing losses, which will allow us to visualize how the efficacy of the model changes as the number of features increases. Unlike our previous experiment, we use the square activation function, which we have found through trial-and-error to be more effective in demonstrating double descent.\n\nmax_feat = 401\nmse_train = []\nmse_test = []\nfor i in range(max_feat):\n    phi = RandomFeatures(n_features = i, activation = square)\n    phi.fit(X_train)\n    X_train_features = phi.transform(X_train)\n    X_test_features = phi.transform(X_test)\n    LR = MyLinearRegression()\n    opt = OverParameterizedLinearRegressionOptimizer(LR)\n    opt.fit(X_train_features, y_train)\n    tr_loss = LR.loss(X_train_features, y_train)\n    te_loss = LR.loss(X_test_features, y_test)\n    mse_train.append(tr_loss)\n    mse_test.append(te_loss)\n\nWe plot the training loss (left) and the testing loss (right) as a function of the number of features used. In the training loss plot, we observe that as the number of features increases the training loss decreases. This is to be expected, as a greater number of features means more weights are used, allowing for more degrees of freedom in our fitting function. However, this also means that the model is becoming more overfit, as the greater degrees of freedom will allow the function to fit to smaller trends in the data that exist because of noise and will not be present in the testing set. Interestingly enough, the loss drops significantly once we approach \\(p = 100\\) features, which is where the number of features is equal to the number of observations. While we would expect the loss to continue to decrease, this large jump in the loss is unexpected and indicates some change in the behavior of the model fit.\nOur suspicions are confirmed when we observe the testing loss. As the number of features increases to \\(p = 100\\), the testing loss increases, which we would expect because a more overfit model will not generalize as well to unique testing data. However, as we increase the number of features past \\(p = 100\\), the loss counterintiuitively begins to decrease, reaching values lower than we observed prior to \\(p = 100\\). This illustrates the double descent that we have been looking for. When we add enough features, the model becomes so overfit that it is no longer overfit and is actually more accurate than it was originally.\n\nfig, ax = plt.subplots(1,2, figsize = (10,4))\nfeat = np.linspace(0,max_feat - 1, max_feat)\nax[0].scatter(feat, mse_train, color = 'gray', s = 12)\nax[1].scatter(feat, mse_test, color = 'darkred', s = 12)\nlabs = ['training', 'testing']\nfor i in range(len(ax)):\n    ax[i].set_yscale('log')\n    ax[i].set_xlabel('Number of features')\n    ax[i].set_ylabel('Mean squared error ({})'.format(labs[i]))\n    ax[i].grid(True)\n    ax[i].axvline(100, color = 'black', ymax = 0.15, lw = 3)\n\n\n\n\n\n\n\n\nWe calculate the feature at which the minimum testing loss occurs using the code below. We find that the lowest testing loss is achieved when we use \\(373\\) features, inidicating that the most efficient model is actually achieved when the number of features exceeds the number of observations rather than when we keep the number of features small as was originally expected.\n\nfeat[np.where(np.array(mse_test) == np.array(mse_test).min())][0]\n\n373.0\n\n\n\n\nConclusion\nThrough our experiments with our implemented linear regression model, we are able to design a set of experiements to observe double descent in action. By creating a linear regression model that can handle overparameterization, we are able to increase the number of parameters to exceed the number of observations, which allows us to achieve testing loss values lower than any testing loss achieved by using fewer features than observations.\nWhile there is still some uncertainty as to why this works, it is likely that the large number of parameters allows the model to perfectly fit the training data while still having enough parameters left over to generalize the data and tune out the effects of noise (thus revealing the true general form of the data). This phenomenon is extremely advantageous for building effective models and has consequently caused deep learning to dominate the field of machine learning. Because more parameters are almost always better, large models are made more accurate not by improving their algorithms, but by increasing the computing power allocated to them and thus the number of parameters they are able to use.\nThis focus on hardware rather than software has a number of implications. As AI and advanced machine learning algorithms enter the mainstream and become commonplace in technologies like our phones, the minimum hardware requirements increase, often making it more expensive for consumers to access new technology. This also makes it difficult for smaller companies to contribute to AI/ML research, as the scalability of models is almost entirely determined by how many GPUs they can afford. There are also environmental concerns here – the more GPUs are needed to train a model, the more electricity and cooling power are needed to support them, thus putting a strain on our environmental resources. While double descent is a powerful feature of machine learning that can bring algorithms to new levels of accuracy and usefulness, it can also have negative effects (such as those listed above) and should be considered carefully when approaching machine learning reasearch."
  },
  {
    "objectID": "posts/auditing-bias/index.html",
    "href": "posts/auditing-bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Machine learning models show potential to revolutionize automated decision-making processes and have already made their way into many of our daily lives through technologies like personalized recommendations, face identification, and self-driving cars. However, these models can also pose extreme dangers, as the models are only as good as the data they are trained on and can very easily reinforce existing injustices in our society. Even when certain traits like race are omitted from the training process, models can predict these traits from other features and still reinforce societal norms that we are trying to avoid. We demonstrate this by training a Decision Tree Classifier model on a dataset of responses to the 2018 Public Use Microdata Sample survey in New York and use it to predict whether an individual is likely to employed. We omit race from our training features and then audit our model to determine if it is truly independent from race. We examine three methods of fairness: calibration, error rate balance, and statistical parity, as well as examine feasible false negative and false positive rates.\nWe begin by importing the folktables package, which we use to import our data. We specify that we are examining responses from 2018 in the state of New York when fetching our data.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"NY\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\nDownloading data for 2018 1-Year person survey for NY...\n\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000012\n2\n1\n3802\n1\n36\n1013097\n145\n26\n...\n146\n146\n21\n24\n266\n263\n21\n146\n265\n144\n\n\n1\nP\n2018GQ0000040\n2\n1\n2702\n1\n36\n1013097\n43\n21\n...\n6\n42\n43\n7\n40\n6\n43\n40\n42\n6\n\n\n2\nP\n2018GQ0000060\n2\n1\n2001\n1\n36\n1013097\n88\n18\n...\n88\n163\n161\n162\n87\n12\n162\n88\n87\n88\n\n\n3\nP\n2018GQ0000081\n2\n1\n2401\n1\n36\n1013097\n109\n85\n...\n17\n15\n111\n107\n17\n196\n109\n200\n198\n111\n\n\n4\nP\n2018GQ0000103\n2\n1\n1400\n1\n36\n1013097\n83\n19\n...\n81\n12\n80\n154\n12\n80\n12\n83\n152\n154\n\n\n\n\n5 rows × 286 columns\n\n\n\nFor the sake of simplicity, we can select specific features to train our model on. We select the following:\n\nAGEP - Age\nSCHL - Educational attainment\nMAR - Marital status\nRELP - Relationship to reference person\nDIS - Disability\nESP - Employment status of parents\nCIT - Citizenship stats\nMIG - Mobility status\nMIL - Military service\nANC - Ancestry\nNATIVITY - Nativity\nDEAR - Hearing difficulty\nDEYE - Vision difficulty\nDREM - Cognitive difficulty\nSEX - Sex\nRAC1P - Race\nESR - Employment status\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n26\n21.0\n5\n17\n2\nNaN\n5\n1.0\n4.0\n1\n2\n2\n2\n2.0\n2\n1\n6.0\n\n\n1\n21\n20.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n1\n1\n1.0\n\n\n2\n18\n16.0\n5\n17\n2\nNaN\n2\n3.0\n4.0\n1\n1\n2\n2\n2.0\n2\n8\n6.0\n\n\n3\n85\n16.0\n2\n16\n1\nNaN\n1\n1.0\n4.0\n4\n1\n1\n2\n1.0\n2\n1\n6.0\n\n\n4\n19\n19.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n2.0\n2\n1\n1.0\n\n\n\n\n\n\n\nWe use the selected features to define our training dataset but exclude employment status (the target variable) and race, which we claim the model will be able to implicitly determine. We define a problem, which specifies our target and hidden features (employment and race).\nLooking briefly at our data, it appears we have \\(196,967\\) observations (people) measured with \\(15\\) features. We confirm that our created feature, label, and group objects all have the same length, as each entry should correspond to a single person.\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(196967, 15)\n(196967,)\n(196967,)\n\n\nAs is good machine learning practice, we define a testing and training dataset so we can later evaluate how our model performs on unseen data.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\nFor later analysis, we create a function that modifies our training and testing datasets, which only include the training features. We add both the group (race) and label (employment status) columns to the new dataset as well as modify encoded labels to labels that we can understand. For the sake of analyzing sufficiently large racial groups, we sort groups into White, African American, Asian, and Other.\n\nimport pandas as pd\ndef format_data(df, features_to_use, group, labels):\n    new_df = pd.DataFrame(df, columns = features_to_use)\n    new_df['group'] = group\n    new_df['employed'] = labels\n    new_df['SEX'] = new_df['SEX'].replace({1.0: 'Male', 2.0: 'Female'})\n    new_df['group'] = new_df['group'].replace({1.0: 'White', \n                                               2.0: 'African American',\n                                               3.0: 'Other Race',\n                                               4.0: 'Other Race',\n                                               5.0: 'Other Race',\n                                               6.0: 'Asian',\n                                               7.0: 'Other Race',\n                                               8.0: 'Other Race',\n                                               9.0: 'Other Race'})\n\n    return new_df\n\nA quick examination of the training data shows that each of our racial groups are in the same order of magnitude with the exception of the White group, which has significantly more people in it.\n\ndf_train = format_data(X_train, features_to_use, group_train, y_train)\ndf_train.groupby('group').size()\n\ngroup\nAfrican American     19200\nAsian                13633\nOther Race           13904\nWhite               110836\ndtype: int64\n\n\nUsing this dataframe, we can determine the population of the training dataset, which we find to be \\(157573\\) people.\n\npopulation = df_train.shape[0]\npopulation\n\n157573\n\n\nOf these individuals, roughly \\(46.5\\%\\) people are employed. This rate is likely so low because we included children in our data, the majority of whom are unemployed.\n\ndf_train['employed'].mean()\n\n0.4649400595279648\n\n\nWe can also analyze the proportion of each race in our population. As we saw in the population counts above, African Americans, Asians, and Other Racial groups each make up a similar amount of the population with \\(11.0\\%\\), \\(9.3\\%\\), and \\(7.9\\%\\) respectively, while White people make up the remaining \\(71.8\\%\\) of the population.\n\ndf_train[df_train['employed'] == 1].groupby('group').size()/df_train[df_train['employed'] == 1].shape[0]\n\ngroup\nAfrican American    0.109607\nAsian               0.092517\nOther Race          0.079277\nWhite               0.718599\ndtype: float64\n\n\nDespite variances in the size of each racial group, we observe roughly equal employment rates across groups such that \\(41.8\\%\\) of African Americans are employed, \\(49.7\\%\\) of Asians are employed, \\(47.5\\%\\) of Whites are employed, and \\(41.8\\%\\) of individuals in other racial groups are employed. These values will be important later for when we audit our model to determine how consistent the predicted employment rates are with reality.\n\ndf_train.groupby('group')['employed'].mean()\n\ngroup\nAfrican American    0.418229\nAsian               0.497176\nOther Race          0.417722\nWhite               0.474990\nName: employed, dtype: float64\n\n\nIt is also worth checking for intersectional trends by examining what percentage of people are employed grouped by sex as well as race. This can help us determine if race is the only major factor in disparities between groups or if there are other contributing factors. Based on the bar plot below, it appears that a greater proportion of men than women are employed with the exception of the African American group. This indicates that women within each racial group may receive further bias from the algorithm, as it is possible that it will predict unemployment for women at higher rates. This may be more prevalent than bias by race since sex is actually a feature that we give to the model wheras race is not.\n\nimport seaborn as sns\nimport seaborn.objects as so\n\nprop = (df_train[df_train['employed'] == 1].groupby(['group', 'SEX']).size()/df_train.groupby(['group', 'SEX']).size()).reset_index(name = 'p')\np1 = sns.barplot(data = prop,\n                 x = 'group',\n                 y = 'p',\n                 hue = 'SEX',\n                 palette='Set2')\np1.set(title = 'Intersection between race and sex', xlabel = 'Race', ylabel = 'Proportion of group employed')\n\n[Text(0.5, 1.0, 'Intersection between race and sex'),\n Text(0.5, 0, 'Race'),\n Text(0, 0.5, 'Proportion of group employed')]"
  },
  {
    "objectID": "posts/auditing-bias/index.html#by-group-measures",
    "href": "posts/auditing-bias/index.html#by-group-measures",
    "title": "Auditing Bias",
    "section": "By-Group Measures",
    "text": "By-Group Measures\nWe can now look at the same quantities as above but sorted by group. Starting with accuracy, we notice that the model seems to be fairly consistent in its predictions with the exception of Asians who are only predicted correctly \\(80.0\\%\\) of the time compares to the other groups who are predicted correctly roughly \\(82\\%\\) of the time.\n\ndf.groupby(\"group\")['correct_prediction'].mean()\n\ngroup\nAfrican American    0.820896\nAsian               0.795996\nOther Race          0.831117\nWhite               0.826760\nName: correct_prediction, dtype: float64\n\n\nWe calculate the positive and negative counts as by group as well, which we will need for our PPV, FPR, and FNR values.\n\ndf.groupby('group')[['TP', 'TN', 'FP', 'FN']].sum()\n\n\n\n\n\n\n\n\nTP\nTN\nFP\nFN\n\n\ngroup\n\n\n\n\n\n\n\n\nAfrican American\n1800\n2160\n574\n290\n\n\nAsian\n1539\n1165\n537\n156\n\n\nOther Race\n1290\n1648\n402\n195\n\n\nWhite\n11104\n11746\n2955\n1833\n\n\n\n\n\n\n\nCalculating the PPVs, we find that the values are fairly consistent with the White group having the highest value of \\(79.0\\%\\) and the Asian group having the lowest predictive value of \\(74.1\\%\\). This means that \\(79.0\\%\\) of the White people that are predicted to be employed are actually employed and \\(74.1\\%\\) of the Asian people that are predicted to be employed are actually employed.\n\nPPV = df.groupby('group')['TP'].sum() / (df.groupby('group')['TP'].sum() + df.groupby('group')['FP'].sum())\nPPV\n\ngroup\nAfrican American    0.758214\nAsian               0.741329\nOther Race          0.762411\nWhite               0.789814\ndtype: float64\n\n\nCalculating the FPR, we find that Asians are falsly predicted to be employed at a much higher rate than other groups with an FPR of \\(31.6\\%\\). The reamining groups all have FPR values of roughly \\(20\\%\\), indicating that our model may have a bias towards the Asian group.\n\nFPR = df.groupby('group')['FP'].sum()/(df.groupby('group')['FP'].sum() + df.groupby('group')['TN'].sum())\nFPR\n\ngroup\nAfrican American    0.209949\nAsian               0.315511\nOther Race          0.196098\nWhite               0.201007\ndtype: float64\n\n\nThe FNR rates closely resemble the trends seen in the FPR values, with each group having a value of roughly \\(13.5\\%\\) with the exception fo the Asian group, which has an FNR of \\(9.2\\%\\). This reinforces our observation that the model tends to overestimate the employment rate of the Asian group relative to the other groups.\n\nFNR = df.groupby('group')['FN'].sum()/(df.groupby('group')['FN'].sum() + df.groupby('group')['TP'].sum())\nFNR\n\ngroup\nAfrican American    0.138756\nAsian               0.092035\nOther Race          0.131313\nWhite               0.141687\ndtype: float64"
  },
  {
    "objectID": "posts/auditing-bias/index.html#bias-measures",
    "href": "posts/auditing-bias/index.html#bias-measures",
    "title": "Auditing Bias",
    "section": "Bias Measures",
    "text": "Bias Measures\nIn order to check for bias, we can evaluate our model based on three standards: calibration, error rate balance, and statistical parity.\nWe can first examine calibration. A model is well calibrated if the prediction rates are consistent with the actual rates across racial groups. If we were to exclude the Asian group, we could say that our model is approximately calibrated. However, the Asian group has a significantly higher prediction rate than actual rate of employment, indicating poor calibration.\n\n# Calibration\nemployed = df.groupby('group')['employed'].mean()\npredicted_employed = df.groupby('group')['pred_employed'].mean()\npd.concat((predicted_employed, employed), axis = 1, keys = ['Prediction', 'True Rate'])\n\n\n\n\n\n\n\n\nPrediction\nTrue Rate\n\n\ngroup\n\n\n\n\n\n\nAfrican American\n0.492123\n0.433250\n\n\nAsian\n0.611127\n0.498970\n\n\nOther Race\n0.478642\n0.420085\n\n\nWhite\n0.508684\n0.468087\n\n\n\n\n\n\n\nNext we examine error rate parity, which evaluates if the FPR and FNR values are consistent with one another across groups. As with the calibration, three of the groups are fairly consistent with their FPR and FNR values, while the Asian group has a substantially higher FPR and a substantially lower FNR, indicating a poor error rate parity as well.\n\n# Error Rate Parity\npd.concat((FPR, FNR), axis = 1, keys = ['FPR', 'FNR'])\n\n\n\n\n\n\n\n\nFPR\nFNR\n\n\ngroup\n\n\n\n\n\n\nAfrican American\n0.209949\n0.138756\n\n\nAsian\n0.315511\n0.092035\n\n\nOther Race\n0.196098\n0.131313\n\n\nWhite\n0.201007\n0.141687\n\n\n\n\n\n\n\nLastly we can check for statistical parity, which examines if the predicted employment rates between groups are equal to one another. A model that satisfies statistical parity would have an equal proportion of people predicted to be employed across each group. Our model also does not satisfy statistical parity, as the predicted rate of Asian employment is signficantly higher than the rate for the three other racial groups.\n\n# Statistical Parity \ndf.groupby('group')['pred_employed'].mean()\n\ngroup\nAfrican American    0.492123\nAsian               0.611127\nOther Race          0.478642\nWhite               0.508684\nName: pred_employed, dtype: float64\n\n\nBased on the paper “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.” by Alexandra Chouldechova, we can examine how fair our model has the potential to be by finding the optimal combinations of FNR and FPR rates. We can calculate the FPR using the FNR with the equation \\[FPR = \\frac{p}{1 - p}\\frac{1 - PPV}{PPV}(1 - FNR)\\] where \\(p\\) is the prevalence or actual employment rate. As an approximation to calibrate our results, we set the PPV to be the minimum PPV value between groups. In this case, we are only examining the White and Asian groups, so we use the Asian PPV as our value. We can add shading to the White line, which shows acceptable FPR values for the White group if we allow the the PPV of the White group to deviate by \\(\\delta = 0.5\\), \\(0.1\\), and \\(0.125\\) from the PPV of the Asian group. Based on the plot, if we desired to tune our classifier threshold so that the false positive rates were equal between races, we would need to increase the false negative rate of the Asian group to roughly \\(0.2\\). This is not a substantial change, indicating that we could achieve a fair outcome without much of a decrease in the accuracy of the model.\n\nimport matplotlib.pyplot as plt\nPPV_min = PPV[['Asian', 'White']].min()\np = df.groupby('group')['employed'].mean()\nFNR_calc = np.linspace(0, 1, 50)\nFPR_calc = lambda p,FNR, PPV_min: (p/(1-p)) * ((1 - PPV_min)/PPV_min)*(1 - FNR)\nFPRs = [FPR_calc(ps, FNR_calc, PPV_min) for ps in p]\nfor i, FPR in enumerate(FPRs):\n    if(p.index[i] in ['Asian', 'White']):\n        plt.plot(FNR_calc, FPR, label = p.index[i])\n        plt.scatter(FNR[i], FPR[i])\nplt.fill_between(FNR_calc, FPR_calc(p['White'], FNR_calc, PPV_min -0.05),FPR_calc(p['White'], FNR_calc, PPV_min +0.05), alpha = 0.1)\nplt.fill_between(FNR_calc, FPR_calc(p['White'], FNR_calc, PPV_min -0.1),FPR_calc(p['White'], FNR_calc, PPV_min +0.1), alpha = 0.2)\nplt.fill_between(FNR_calc, FPR_calc(p['White'], FNR_calc, PPV_min -0.125),FPR_calc(p['White'], FNR_calc, PPV_min +0.125), alpha = 0.4)\nplt.xlabel('False Negative Rate')\nplt.ylabel('False Positive Rate')\nplt.title('Feasible (FNR, FPR) Combinations')\nplt.legend()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Omar Armbruster’s Machine Learning Blog",
    "section": "",
    "text": "Project Solar-Searcher\n\n\n\n\n\nIdentifying the optimal locations for solar energy system development in the continental US\n\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Optimization: Newton’s Method and Adam\n\n\n\n\n\nImplementations of the Newton and Adam Optimizers\n\n\n\n\n\nMay 5, 2025\n\n\nOmar Armbruster\n\n\n\n\n\n\n\n\n\n\n\n\nObserving Double Descent\n\n\n\n\n\nSo you’re telling me my model is too athletic?\n\n\n\n\n\nApr 16, 2025\n\n\nOmar Armbruster\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nThose penguins won’t know what hit them\n\n\n\n\n\nApr 7, 2025\n\n\nOmar Armbruster\n\n\n\n\n\n\n\n\n\n\n\n\nThe Perceptron\n\n\n\n\n\nIt’s Alive! Awakening the Perceptron\n\n\n\n\n\nApr 2, 2025\n\n\nOmar Armbruster\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nAnalyzing Implicit Bias in Machine Learning Models\n\n\n\n\n\nMar 12, 2025\n\n\nOmar Armbruster\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decisions\n\n\n\n\n\nBorrowers v. Bankers: A case study in maximizing shareholder value\n\n\n\n\n\nMar 5, 2025\n\n\nOmar Armbruster\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins\n\n\n\n\n\nA Positively Preposterous Post Pondering Palmer Penguins\n\n\n\n\n\nFeb 12, 2025\n\n\nOmar Armbruster\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Maybe we were the ones learning all along…"
  },
  {
    "objectID": "posts/advanced-optimization/index.html",
    "href": "posts/advanced-optimization/index.html",
    "title": "Advanced Optimization: Newton’s Method and Adam",
    "section": "",
    "text": "Abstract\nIn this post we examine Newton’s method as an extension of gradient descent as well as the Adam Optimizer. We implement each optimizer and apply them to a logistic regression model to examine their efficacy. We compare Newton’s method to standard gradient descent and the Adam Optimizer to stochastic gradient descent on both simulated data and the empirical titanic dataset to evaluate their performances. We then compare Adam and Newton by determining how long it takes each to achieve a loss of \\(0.5\\). We find that Newton’s method is much faster than Adam for multiple learning rates. The code for the optimizers can be found here.\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\n\n\n%load_ext autoreload\n%autoreload 2\nfrom optimizers import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer, AdamOptimizer, StochasticGradientDescent\n\nWe begin our experiments by defining a function to easily generate two-dimensional data with gaussian noise. This will allow us to test how well our optimizers work before applying them to more complex empirical data.\n\n## Sourced from Phil Chodrow Lecture Notes\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.2)\n\nWe also define plotting functions to allow us to visualize the decision regions and loss over numerous iterations of the algorithm. This will allwo us to easily evaluate the efficacy and efficiency of each optimizer.\n\n## Data plotting function sourced from Phil Chodrow Lecture Notes\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"PuOr\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n## Boundary drawing function sourced from Phil Chodrow Lecture Notes\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef plot_loss(loss, ax, **kwargs):\n    ax.plot(loss, **kwargs)\n    ax.set_xlabel('Iteration')\n    ax.set_ylabel('Loss')\n\ndef plot_results(LR, X, y, loss, ax, **kwargs):\n    plot_data(X,y,ax[0])\n    draw_line(LR.w, -1, 2, ax[0], color = 'black', **kwargs)\n    ax[0].set_title('Decision Boundary')\n    ax[1].set_title('Loss')\n    plot_loss(loss, ax[1], **kwargs)\n\n\n\nNewton’s Method\nWe first implement Newton’s method, which modifies standard gradient descent by multiplying the gradient function by the Hessian matrix, which we compute by \\[h_{ij}(\\textbf{w}) = \\sum_k^2 x_{ki}x_{kj}\\sigma(s_k)(1 - \\sigma(s_k)).\\] Using this definition we can then update the weights by \\[w^{t + 1} = w^t - \\alpha H(\\textbf{w})^{-1}\\nabla L(\\textbf{w})\\] where \\(\\alpha\\) is the learning rate and \\(H\\) is the Hessian matrix. We can compare this to standard gradient descent by training both for \\(1000\\) iterations with a learning rate of \\(\\alpha = 0.1\\). We also add a \\(w^2\\) regularization term, which we scale with parameter lam, which we set to \\(0.01\\). It is important here that the regularaization term is less than the learning rate so that the weights are primarily determined by the gradient rather than the regularization.\n\nLR_G = LogisticRegression() \nopt = GradientDescentOptimizer(LR_G)\n\n\nloss_G = []\nfor _ in range(1000):\n    l = LR_G.loss(X,y, lam = 0.01)\n    loss_G.append(l)\n    opt.step(X, y, alpha = 0.1, beta = 0, lam = 0.01)\n\n\nLR_N = LogisticRegression() \nopt = NewtonOptimizer(LR_N)\n\n\nloss_N = []\nfor _ in range(1000):\n    l = LR_N.loss(X,y, lam = 0.01)\n    loss_N.append(l)\n    opt.step(X, y, alpha = 0.1, lam = 0.01)\n\nWe plot the results for both models and find that both are able to achieve perfect accuracy and very low loss on the linearly separable simulate data. While both optimizers converge to the same loss value, it appears that Newton converges much faster than the standard gradient descent.\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 5))\nfig.suptitle('Newton and GD')\nplot_results(LR_G, X, y, loss_G, ax, label = 'Gradient Descent')\nplot_results(LR_N, X, y, loss_N, ax, label = 'Newton Optimizer', ls = ':')\nax[0].legend()\nax[1].legend()\n\n\n\n\n\n\n\n\n\n\nNewton on Empirical Data\nTo test the true strength of our Newton optimizer, we apply it to empirical data. For this case we use the Titanic dataset from the seaborn package. Our goal is to use the provided features to determine whether or not a given passenger was likely to have survived the sinking of the Titanic. Taking a quick look at the data, we can see features like sex, age, number of siblings/spouses aboard, and the class of ticket the passenger possessed, which each could have some sort of indication on whether a given passenger was more or less likely to survive.\n\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nempirical_data = sns.load_dataset('titanic')\nempirical_data.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\nIn order for our logisitic regression model to be able to process the data, we need to convert categorical variables to dummies as well as drop redundant columns and columns that might give away the true label. We also scale the continuous variables using a standard scaling so that all features are considered equally.\n\ne_data = empirical_data.dropna(axis = 0)\ny_e = e_data['survived']\ne_data = e_data.drop(['survived', 'adult_male', 'alone', 'alive', 'pclass'], axis = 1)\nX_e = pd.get_dummies(e_data)\n# X_e['ones'] = np.ones_like(y_e)\nX_e['age'] = (X_e['age'] - X_e['age'].mean())/X_e['age'].std()\nX_e['fare'] = (X_e['fare'] - X_e['fare'].mean())/X_e['fare'].std()\nX_e.head()\n\n\n\n\n\n\n\n\nage\nsibsp\nparch\nfare\nsex_female\nsex_male\nembarked_C\nembarked_Q\nembarked_S\nclass_First\n...\ndeck_A\ndeck_B\ndeck_C\ndeck_D\ndeck_E\ndeck_F\ndeck_G\nembark_town_Cherbourg\nembark_town_Queenstown\nembark_town_Southampton\n\n\n\n\n1\n0.151664\n1\n0\n-0.099835\n1\n0\n1\n0\n0\n1\n...\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n-0.039765\n1\n0\n-0.337554\n1\n0\n0\n0\n1\n1\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n\n\n6\n1.172618\n0\n0\n-0.353732\n0\n1\n0\n0\n1\n1\n...\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n\n\n10\n-2.017864\n1\n1\n-0.813428\n1\n0\n0\n0\n1\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n11\n1.427856\n0\n0\n-0.684654\n1\n0\n0\n0\n1\n1\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n5 rows × 25 columns\n\n\n\nWe apply the train-test split, allowing us to validate our models after trianing them.\n\nX_train, X_test, y_train, y_test = train_test_split(X_e,y_e, test_size=0.3)\nX_train = torch.tensor(X_train.to_numpy(), dtype = torch.float32)\ny_train = torch.tensor(y_train.to_numpy(), dtype = torch.float32)\nX_test = torch.tensor(X_test.to_numpy(), dtype = torch.float32)\ny_test = torch.tensor(y_test.to_numpy(), dtype = torch.float32)\n\nJust as with the simulated data, we train our gradient descent and Newton optimizers for \\(1000\\) iterations with a learning rate of \\(\\alpha = 0.1\\).\n\nLR_G = LogisticRegression() \nopt = GradientDescentOptimizer(LR_G)\n\n\nloss_G = []\nfor _ in range(1000):\n    l = LR_G.loss(X_train,y_train, lam = 0.01)\n    loss_G.append(l)\n    opt.step(X_train, y_train, alpha = 0.1, beta = 0, lam = 0.01)\n\n\nLR_N = LogisticRegression() \nopt = NewtonOptimizer(LR_N)\n\n\nloss_N = []\nfor _ in range(1000):\n    l = LR_N.loss(X_train,y_train, lam = 0.01)\n    loss_N.append(l)\n    opt.step(X_train, y_train, alpha = 0.1, lam = 0.01)\n\nBecause there are more than two features, we cannot plot the decision boundaries. However, we observe a similar trend to the simulated data in the loss, where both converge to a low loss value with Newton’s method converging more quickly. This means that with an adequately selected value of \\(\\alpha\\), Newton’s method can converge to the correct value of \\(\\textbf(w)\\) and in some cases, can converge faster than gradient descent.\n\nfig, ax = plt.subplots(1,1)\nplot_loss(loss_G, ax, label = 'Gradient Descent')\nplot_loss(loss_N, ax, label = 'Newton Optimizer', ls = ':')\nplt.legend()\n\n\n\n\n\n\n\n\nWe can validate the training of our models and find that both achieve very similar testing losses, indicating that neither model was overfit, even though they converged quite quickly.\n\nprint(\"Gradient Descent Test Loss: {} Newton Test Loss: {}\".format(LR_G.loss(X_test, y_test).item(), LR_N.loss(X_test, y_test).item()))\n\nGradient Descent Test Loss: 0.46767881512641907 Newton Test Loss: 0.46853649616241455\n\n\nWe can also test the case where \\(\\alpha\\) is selected to be too high, in which we expect Newton’s method to fail. We train our two models as we did in the previous experiments.\n\nLR_G = LogisticRegression() \nopt = GradientDescentOptimizer(LR_G)\n\n\nloss_G = []\nfor _ in range(100):\n    l = LR_G.loss(X_train,y_train, lam = 0)\n    loss_G.append(l)\n    opt.step(X_train, y_train, alpha = 3.5, beta = 0, lam = 0)\n\nLR_N = LogisticRegression() \nopt = NewtonOptimizer(LR_N)\n\n\nloss_N = []\nfor _ in range(100):\n    l = LR_N.loss(X_train,y_train, lam = 0)\n    # print(LR_N.w)\n    # print(LR_N.score(X_train))\n    loss_N.append(l)\n    opt.step(X_train, y_train, alpha = 3.5, lam = 0)\n\nWhen we plot the loss, we find that standard gradient decent is still able to converge (although much slower) while Newton’s method does not converge at all, oscillating between high loss values near the starting loss. This means that while Newton’s method can be quite powerful, it is also fallabile and some cases.\n\nfig, ax = plt.subplots(1,1)\nplot_loss(loss_G, ax, label = 'Gradient Descent')\nplot_loss(loss_N, ax, label = 'Newton Optimizer', ls = ':')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nAdam Optimizer\nWe also implement the Adam Optimizer, a common optimizer used in deep learning models, behaving very similarly to stochastic gradient descent (SGD). The Adam Optimizer randomly selects batches whose size are determined by the batch_size parameter and then updates the weights such that \\[w^{t + 1} = w^t - \\alpha \\frac{\\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon}\\] where \\(\\hat{m}\\) and \\(\\hat{v}\\) are variables based on the gradient that change with time (the number of iterations) and constants \\(\\beta_1\\) and \\(\\beta_2\\). This time dependent behavior means the step size will decrease with each iteration, allowing for a more precise convergence on the optimal weights near the last few iterations. We train our models with the Adam and SGD optimizers using a learning rate of \\(\\alpha = 0.01\\). In order to fine tune our models, we use a very small regularization term for Adam and no regularization for SGD.\n\nLR_A = LogisticRegression() \nopt = AdamOptimizer(LR_A, 10, 0.01, 0.9, 0.999)\n\n\nloss_A = []\nfor _ in range(1000):\n    l = LR_A.loss(X,y, lam = 0.0001)\n    loss_A.append(l)\n    # print(LR_A.w)\n    opt.step(X, y, lam = 0.0001)\n\nLR_SG = LogisticRegression() \nopt =StochasticGradientDescent(LR_SG, 10, alpha = 0.01)\n\n\nloss_SG = []\nfor _ in range(1000):\n    l = LR_SG.loss(X,y, lam = 0)\n    loss_SG.append(l)\n    opt.step(X, y, lam = 0)\n\nWhen we plot the fit of these two optimizers, we find that both are able to achieve comparable decision boundaries and final losses, but with Adam converging more quickly than the standard SGD.\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 5))\nfig.suptitle('Adam and SGD')\nplot_results(LR_A, X, y, loss_A, ax, label = 'Adam')\nplot_results(LR_SG, X, y, loss_SG, ax, label = 'Stochastic Gradient Descent', ls = ':')\nax[0].legend()\nax[1].legend()\n\n\n\n\n\n\n\n\nWe compare these two optimizers on the Titanic data as well for three values of \\(\\alpha\\). For \\(\\alpha = 0.001\\), we find that both begin to converge (although do not fully converge after \\(1000\\) iterations) but the SGD converges at a slower rate than the Adam optimizer.\n\nLR_A = LogisticRegression() \nopt = AdamOptimizer(LR_A, 10, 0.001, 0.9, 0.999)\n\n\nloss_A = []\nfor _ in range(1000):\n    l = LR_A.loss(X_train,y_train, lam = 0.0001)\n    loss_A.append(l)\n    # print(LR_A.w)\n    opt.step(X_train, y_train, lam = 0.0001)\n\nLR_SG = LogisticRegression() \nopt =StochasticGradientDescent(LR_SG, 10, alpha = 0.001)\n\n\nloss_SG = []\nfor _ in range(1000):\n    l = LR_SG.loss(X_train,y_train, lam = 0.0001)\n    loss_SG.append(l)\n    opt.step(X_train, y_train, lam = 0.0001)\n\nfig, ax = plt.subplots(1,1)\nax.set_title(r'Adam vs SGD $\\alpha = 0.001$')\nplot_loss(loss_A, ax, label = 'Adam Optimizer')\nplot_loss(loss_SG, ax, label = 'Stochastic Gradient Descent', ls = ':')\nplt.legend()\n\n\n\n\n\n\n\n\nWith a higher learning rate of \\(\\alpha = 0.01\\), we observe a similar convergance but in much fewer iterations as the steps towards the optimal weights at each step will be larger.\n\nLR_A = LogisticRegression() \nopt = AdamOptimizer(LR_A, 10, 0.01, 0.9, 0.999)\n\n\nloss_A = []\nfor _ in range(1000):\n    l = LR_A.loss(X_train,y_train, lam = 0.01)\n    loss_A.append(l)\n    # print(LR_A.w)\n    opt.step(X_train, y_train, lam = 0.01)\n\nLR_SG = LogisticRegression() \nopt =StochasticGradientDescent(LR_SG, 10, alpha = 0.01)\n\n\nloss_SG = []\nfor _ in range(1000):\n    l = LR_SG.loss(X_train,y_train, lam = 0.01)\n    loss_SG.append(l)\n    opt.step(X_train, y_train, lam = 0.01)\n\nfig, ax = plt.subplots(1,1)\nax.set_title(r'Adam vs SGD $\\alpha = 0.01$')\nplot_loss(loss_A, ax, label = 'Adam Optimizer')\nplot_loss(loss_SG, ax, label = 'Stochastic Gradient Descent', ls = ':')\nplt.legend()\n\n\n\n\n\n\n\n\nWhen we use \\(\\alpha = 0.1\\) for the two optimizers, we find that Adam still converges faster but appears to converge at a higher loss value than SGD. Like Newton, speed appears to come at the expense of accuracy when \\(\\alpha\\) is too large, meaning we need to be more cognicant of the learning rate we choose when using these more powerful optimizers.\n\nLR_A = LogisticRegression() \nopt = AdamOptimizer(LR_A, 10, 0.1, 0.9, 0.999)\n\n\nloss_A = []\nfor _ in range(1000):\n    l = LR_A.loss(X_train,y_train, lam = 0.001)\n    loss_A.append(l)\n    # print(LR_A.w)\n    opt.step(X_train, y_train, lam = 0.001)\n\nLR_SG = LogisticRegression() \nopt =StochasticGradientDescent(LR_SG, 10, alpha = 0.1)\n\n\nloss_SG = []\nfor _ in range(1000):\n    l = LR_SG.loss(X_train,y_train, lam = 0.0)\n    loss_SG.append(l)\n    opt.step(X_train, y_train, lam = 0.0)\n\nfig, ax = plt.subplots(1,1)\nax.set_title(r'Adam vs SGD $\\alpha = 0.1$')\nplot_loss(loss_A, ax, label = 'Adam Optimizer')\nplot_loss(loss_SG, ax, label = 'Stochastic Gradient Descent', ls = ':')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nNewton vs Adam\nLastly we want to compare our two implemented optimizers. Becuase the computations are so different between the two methods, we want to measure the different in time to achieve a loss value rather than the number of iterations. We can test this for multiple values of \\(\\alpha\\) by determining the amount of time it takes to achieve a loss of \\(0.5\\) or hit \\(1000\\) iterations (to avoid an infinite loop). We print the times for each below and find that at lower values of \\(\\alpha\\), Adam appears to be faster. However, as \\(\\alpha\\) increases, the times become closer and closer before Newton becomes the faster optimizer at \\(\\alpha = 1\\). Based on the times below, Adam is fastest at \\(\\alpha = 0.1\\), taking \\(0.008\\) seconds to converge. However, from our previous experiment, we know that when \\(\\alpha = 0.1\\), Adam converges at a fairly high loss value. This means that there is some tradeoff between speed and accuracy to consider when choosing the optimal learning rate.\n\nalphas = [0.001, 0.01, 0.1, 1]\n\nfor alpha in alphas:\n    print('--------alpha = {}--------'.format(alpha))\n\n\n    LR_A = LogisticRegression() \n    opt = AdamOptimizer(LR_A, 10, alpha, 0.9, 0.999)\n\n    \n    loss_A = []\n    l = LR_A.loss(X_train,y_train)\n    count = 0\n    start = time.time()\n    while l &gt; 0.5 and count &lt; 1000:\n        l = LR_A.loss(X_train,y_train)\n        loss_A.append(l)\n        # print(LR_A.w)\n        opt.step(X_train, y_train, lam = alpha/10)\n        count += 1\n    end = time.time()\n    print('Adam Optimizer Time: {}'.format(end - start))\n\n    LR_N = LogisticRegression() \n    opt = NewtonOptimizer(LR_N)\n\n    loss_N = []\n    l = LR_N.loss(X_train,y_train)\n    count = 0\n    start = time.time()\n    while l &gt; 0.5 and count &lt; 1000:\n        l = LR_N.loss(X_train,y_train)\n        loss_N.append(l)\n        opt.step(X_train, y_train, alpha = alpha/10)\n        count += 1\n    end = time.time()\n    print('Newton Optimizer Time: {}'.format(end - start))\n\n--------alpha = 0.001--------\nAdam Optimizer Time: 0.2750883102416992\nNewton Optimizer Time: 0.7343175411224365\n--------alpha = 0.01--------\nAdam Optimizer Time: 0.018273591995239258\nNewton Optimizer Time: 0.32607245445251465\n--------alpha = 0.1--------\nAdam Optimizer Time: 0.008029460906982422\nNewton Optimizer Time: 0.030339717864990234\n--------alpha = 1--------\nAdam Optimizer Time: 0.040769338607788086\nNewton Optimizer Time: 0.0\n\n\n\n\nDiscussion\nThis post examined the implementations of the Newton and Adam optimizers and analyzed how they comapared to their simpler gradient descent and stochastic gradient descent counterparts. We found that in most cases, the more advanced optimizers converged faster than the simpler versions. However, in the case of high learning rates, the simpler versions presented more durability, as Newton would oscillate at a high loss value and not converge at all, while Adam would converge but at a much higher loss value than SGD. We tested these observations on both simulated linearly separable data as well as empirical data on the Titanic dataset. Lastly, we compared the two advanced optimizers by comparing the speed at which they achieved a loss of \\(0.5\\). The faster optimizer was largely dependent on the learning rate. However, because we were testing convergance to a fairly high loss value, we need to consider accuracy as well as speed when choosing the optimal optimizer and learning rate combination. Based on this post, we can conclude that both optimizers have their own strengths and weaknesses which can be used to fit the varying needs od different datasets."
  },
  {
    "objectID": "posts/automated-decision/index.html",
    "href": "posts/automated-decision/index.html",
    "title": "Design and Impact of Automated Decisions",
    "section": "",
    "text": "Abstract\nThis post examines threshold-choosing techniques that allow us to evaluate the efficacy of a classifier outside of the accuracy. Our case examines bank lending where our machine learning model must evaluate if a borrower is likely to default on their loans. In this scenario, it is much more costly for the bank to approve a loan that will be defaulted on than to reject a borrower that would have actually been able to pay back their loan. Because the errors are asymmetric in cost, we can find an optimal threshold for approving and denying loans, which will allow the bank to maximize profits. We use logistic regression on our dataset to find an optimal set of weights and then use the dot product to use these weights to convert our data to a probability that each prosepctive borrower will default. Based on these probabilities, we test a set of thresholds for allowable default risk and select the optimal one that will maximize profit. We find that the optimal threshold rejects any prospective borrower with a risk of default above \\(58\\%\\) and results in a profit of roughly \\(\\$ 1400\\) per prospective borrower. We also analyze how our threshold impacts different demographics of borrowers.\n\n\nIntroduction\nTo loan or not to loan: That is the question which most often plagues the indecisive banker. For the siren call of unlimited riches in the form of monthly interest payments beckons, but can lead to certain doom for those who make hasty loans that end in default. So the banker is left with a choice: lay down anchor and decry all loans to avoid disaster or set sail into the perilous loan-giving waters in search of the glorious bounty that awaits. Fortunately for today’s bankers, our modern day Prometheus–Big Tech–has bestowed upon us machines, which can make decisions autonomously, providing some solace for those who do not wish to bear the decision-making responsibility alone.\nWe can train one such machine learning model as follows, beginning by importing our trusty tools NumPy and Pandas as well as our training dataset. Our interest rates are given as percentages, which we will want to convert to a decimal for later calculations.\n\nimport pandas as pd\nimport numpy as np\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train['loan_int_rate'] = df_train[\"loan_int_rate\"]/100 # Convert percentage to decimal\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n0.0991\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n0.1347\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n0.0751\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n0.1287\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n0.0963\n0\n0.28\nN\n10\n\n\n\n\n\n\n\nWe preprocess the data in a way that can be read by our machine by converting our categorical columns into dummy columns and dropping NA values. We also separate our target variable “loan_status”, which is a \\(1\\) if the borrower defaulted and a \\(0\\) if the borrower paid back their loan. “loan_grade” is a measurement from the bank that determines the likelyhood that the loan is paid back. We want to remove this as well, as the goal is for our model to not be under the influence of outside evaluation methods.\n\ndef preprocess_data(df):\n    df = df.dropna()\n    y_train = df[\"loan_status\"]\n    df = df.drop([\"loan_status\", \"loan_grade\"], axis = 1)\n    df = pd.get_dummies(df)\n    return df, y_train\n\nX_train, y_train = preprocess_data(df_train)\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n0.1347\n0.12\n6\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n0.0751\n0.27\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n0.1287\n0.05\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n0.0963\n0.28\n10\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n0.1491\n0.25\n2\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\n\nExploring The Data\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nOur model is only as good as the data it is trained on, and as such, it is important for us to gain a strong understanding of our data before beginning the training process. The scatterplot on the left examines the relationship between two continuous features: “loan_percent_income” and “loan_int_rate”, representing the percentage of the borrower’s income that the requested loan consitutes and the the interest rate of the loan, respectively. Based on the scatterplot–which is colored by if the borrower defaulted–it appears that as both variables increase, the rate of default also increases.\nThe barplot to the right examines the homeowner status of borrowers and the rate at which each of those groups default on their loans. Borrowers who were renters and had a status of “Other” had the highest rates of default at around \\(30\\%\\), while borrowers who had a mortgage or owned their homes had lower default rates of around \\(12\\%\\) and \\(9\\%\\), respectively.\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 4))\nplt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.3, hspace=0.4)\np1 = sns.scatterplot(df_train, x = \"loan_percent_income\", y = \"loan_int_rate\", hue = \"loan_status\", ax = ax[0])\np2 = sns.barplot(df_train, x = \"person_home_ownership\", y = \"loan_status\", ax = ax[1])\n\n\n\n\n\n\n\n\nUsing the table below, we can examine the outcomes for groups with varying intents for their loans. It appears that on average, the interest rate granted by the bank was around \\(11\\%\\), regardless of what the intent was for the loan. Venture loans had the lowest rate of default with a mean of \\(14.9\\%\\) while debt consolidation loans had the highest rate of default with a mean of \\(28.7\\%\\).\n\ndf_train.groupby(['loan_intent']).aggregate({'loan_int_rate': ['mean', 'std'],\n                                             'loan_status': ['mean', 'std']})\n\n\n\n\n\n\n\n\nloan_int_rate\nloan_status\n\n\n\nmean\nstd\nmean\nstd\n\n\nloan_intent\n\n\n\n\n\n\n\n\nDEBTCONSOLIDATION\n0.109833\n0.032782\n0.287458\n0.452631\n\n\nEDUCATION\n0.109655\n0.031817\n0.173396\n0.378626\n\n\nHOMEIMPROVEMENT\n0.111601\n0.033750\n0.264645\n0.441220\n\n\nMEDICAL\n0.110519\n0.032451\n0.263289\n0.440463\n\n\nPERSONAL\n0.110098\n0.032337\n0.193739\n0.395271\n\n\nVENTURE\n0.109409\n0.032190\n0.148678\n0.355809\n\n\n\n\n\n\n\n\n\nBuilding The Model\nThis is where the fun begins: we can now build a model that will help us make loans that optimize our profits. We’ve chosen ol’ reliable–Logistic Regression–as our model, which we can fit to a subset of the features in our data. “But which features?” you may be wondering. Great question! We can determine an ideal combination of features through a trial-and-error like process by fitting a model to different combinations of features and then performing cross-validation on each to determine which set will perform well on a new testing dataset. We define a function getCols() below to make sure all of our relevant categorical dummies are selected when choosing our features.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\ndef getCols(X, quant_cols, qual_cols):\n  cols = quant_cols\n  for qual in qual_cols: \n    expand_qual_cols = [col for col in X.columns if qual in col ]\n    cols = np.concatenate((cols, expand_qual_cols))\n  return cols\n\nOur first combination of features uses both of the categorical variables we examined in our plots above as well as the interest rate, percentage of the borrowers income that the loan constitutes, the borrower’s age, and the employment length of the borrower’s most recent occupation. Based on our cross-validation test, these features score \\(84.4\\%\\). Nice!\n\nqual_cols = ['person_home_ownership', 'loan_intent']\nquant_cols = ['loan_int_rate', 'loan_percent_income', 'person_age', 'person_emp_length']\ncols1 = getCols(X_train, quant_cols, qual_cols)\nLR = LogisticRegression()\nLR.fit(X_train[cols1], y_train)\ncross_val_score(LR, X_train[cols1], y_train, cv = 5).mean()\n\n0.8439336903961869\n\n\nOur second combination ignores all categorical variables and just focuses on the interest rate and the loan’s percentage of the borrower’s income, which could be effective based on the trend we saw in the scatterplot above. We find a cross-validation score of \\(82.6\\%\\). While not as high as the previous combination, the cross-validation score is quite high, indicating a strong predictive power between these two features.\n\nqual_cols = []\nquant_cols = ['loan_int_rate', 'loan_percent_income']\ncols2 = getCols(X_train, quant_cols, qual_cols)\nLR = LogisticRegression()\nLR.fit(X_train[cols2], y_train)\ncross_val_score(LR, X_train[cols2], y_train, cv = 5).mean()\n\n0.825773775136919\n\n\nOur last combination uses the same features as the first combination but ignores the loan interest rate and the loan’s percentage of the borrower’s income. Using cross-validation, we get a score of \\(78.5\\%\\).\n\nqual_cols = ['person_home_ownership', 'loan_intent']\nquant_cols = ['person_age', 'person_emp_length']\ncols3 = getCols(X_train, quant_cols, qual_cols)\nLR = LogisticRegression()\nLR.fit(X_train[cols3], y_train)\ncross_val_score(LR, X_train[cols3], y_train, cv = 5).mean()\n\n0.7849565667540506\n\n\nWhile accuracy is not the only consideration to be made when profitmaxxing (as we will get to in a moment), a higher accuracy and cross-validation score indicate a strength in our model that we will want when trying to minimize certain types of error. As such, we can select the first combination of features we tested, which include the loan intent, borrower’s homeowner status, the interest rate, the loan’s percentage of the borrower’s income, the borrower’s age, and the borrower’s employment history.\n\ncols = cols1\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.8458986336054481\n\n\nWith our trained model, we can extract the component of greatest interest (bank pun?): the weights, which indicate how much each feature should be considered. Using the weights, we can convert any set of borrower data into a set of probabilities representing the predicted probability that a given borrower will default on their loans.\n\nweights = LR.coef_[0]\nweights\n\narray([18.18778916,  9.26840527, -0.03010567, -0.0267996 , -0.66674985,\n       -0.4983932 , -1.81653014,  0.08564683, -0.04352733, -1.01633329,\n       -0.14003246, -0.1609493 , -0.6349797 , -0.90020427])\n\n\n\n\nOptimizing a Threshold\nUsing our calculate_norm_scores() function, we can calcuate the predicted probability \\(s\\) that each borrower in our dataset will default on their loans. This works by finding the dot product between the feature data and their corresponding weights and then using min-max scaling to set the dot products on a scale from \\(0\\) to \\(1\\). We can interpret these values as probabilities where a \\(1\\) represents a \\(100\\%\\) chance of defaulting and a \\(0\\) represents a \\(0\\%\\) chance of defaulting.\n\ndef calculate_norm_scores(X, weights):\n    unNorm = X@weights\n    return (unNorm - unNorm.min())/(unNorm.max() - unNorm.min())\ns = calculate_norm_scores(X_train[cols], weights)\n\nWe can plot the probabilities on a histogram to observe the distribution of the borrowers. The data resemble a gaussian where the average borrower appears to have around a \\(40\\%\\) chance of default.\n\n\nCode\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nhist = ax.hist(s, bins = 50, color = \"steelblue\", alpha = 0.6, linewidth = 1, edgecolor = \"black\")\nlabs = ax.set(xlabel = r\"Score $s$\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\nFigure 1: Histogram of scores. Code sourced from Phil Chodrow Lecture Notes.\n\n\n\n\n\nNow to choose our threshold. To find the optimal risk cutoff, we can employ the very elegant and efficient brute force approach. We can test \\(100\\) values of our threshold \\(t\\) between \\(0\\) and \\(1\\), calculating whether each borrower falls above or below the threshold. Borrowers who fall above the threshold are considered to be too high of a risk to give a loan to, while borrowers below the threshold will be approved to receive a loan. We can then compare these labels to the real labels from the “loan_status” column. Based on this, we can calculate the total profit of the bank. If the model predicts that a borrower will pay back their loan and the borrower actually does, the bank will make a profit of \\[loan\\_amnt*(1+0.25*int\\_rate)^{10} - loan\\_amnt\\] which assumes that the bank profits \\(25\\%\\) of the interest rate after paying operating expenses and that the loan is paid back in ten years. If the model predicts that the borrower will pay back their loans and they actually default, the bank’s profit can be calculated by \\[loan\\_amnt*(1 + 0.25*int\\_rate)^3 - 1.7*loan\\_amnt\\] which assumes that the borrower defaults after three years causing the bank to lose \\(70\\%\\) of the principal. This amount is typically a negative value. In the banking industry this is considered to be “bad for business”. We can sum the values of these two profits to determine the total profit of the bank. We can calculate the profit yielded by each threshold and then set our best threshold to be whichever yields the highest profit. In our figure below, it appears optimal to reject any borrowers with a risk of default higher than \\(58\\%\\), which yields a profit of \\(\\$1452.12\\) per prospective borrower. Not too bad!\n\n# Figure code sourced from Phil Chodrow Lecture Notes and modified for the algorithm used in this post.\n\nbest_benefit = 0\nbest_threshold = 0\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nfor t in np.linspace(0, 1, 101): \n    y_pred = s &gt;= t\n    tn = np.sum((X_train['loan_amnt']*(1 + 0.25*X_train['loan_int_rate'])**10 - X_train['loan_amnt'])[(y_train == 0)&(y_pred == 0)])\n    fn = np.sum((X_train['loan_amnt']*(1 + 0.25*X_train['loan_int_rate'])**3 - 1.7*X_train['loan_amnt'])[(y_train == 1)&(y_pred == 0)])\n    benefit = (tn + fn)/len(X_train)\n    ax.scatter(t, benefit, color = \"steelblue\", s = 10)\n    if benefit &gt; best_benefit: \n        best_benefit = benefit\n        best_threshold = t\n\nax.axvline(best_threshold, linestyle = \"--\", color = \"grey\", zorder = -10)\nlabs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Benefit per Prospective Borrower\", title = f\"Best Benefit of ${best_benefit:.2f} Per Prospective Borrower at Threshold t = {best_threshold:.3f}\")\n\n\n\n\n\n\n\n\n\n\nEvaluating The Model as a Banker\nSo how does our model perform on a testing dataset where the model weights have not been influenced by the data it is transforming? Let’s find out!\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test[\"loan_int_rate\"] = df_test[\"loan_int_rate\"]/100\nX_test, y_test = preprocess_data(df_test)\n\nUsing the same features we used before and the weights from our trained Logistic Regression model, we can transform the testing data into a set of probabilities as well.\n\ns = calculate_norm_scores(X_test[cols], weights)\n\nUsing the best threshold we found to be optimal from the training data, we can assign labels to each of our testing borrowers and calculate the bank’s total profit based on these predictions. We find that the bank profits \\(\\$1366.84\\) per prospective borrower, which is very similar to the profits we found with the training data. Success!\n\ny_pred = s &gt;= best_threshold\ntn = np.sum((X_test['loan_amnt']*(1 + 0.25*X_test['loan_int_rate'])**10 - X_test['loan_amnt'])[(y_test == 0)&(y_pred == 0)])\nfn = np.sum((X_test['loan_amnt']*(1 + 0.25*X_test['loan_int_rate'])**3 - 1.7*X_test['loan_amnt'])[(y_test == 1)&(y_pred == 0)])\nbenefit = (tn + fn)/len(X_test)\nbenefit\n\n1366.8358295680039\n\n\n\n\nEvaluating the Model as a Borrower\nWhile optimizing profits can certaintly be exhilerating, it does not come without its costs. Because the model cannot predict with complete certainty whether a borrower will default, there are bound to be borrowers who are denied loans even if they actually would have been able to pay back their loans. In the name of fairness, we can examine who was approved and denied loans and determine if certain groups of people are advantaged or disadvantaged by our model.\nWe first look at the average prediction based on the borrowers age. Our model predicts that borrowers from ages \\(20\\) to \\(30\\) will have the highest rate of default at \\(14.4\\%\\). While this does disadvantage young people, the model still predicts the rate of default to be much lower than the actual rate, meaning many young people that are approved a loan will still default. The greatest disparity in predictions is in the \\(50\\) to \\(60\\) year old age group, where only \\(2.9\\%\\) are predicted to default when this group actually has the highest rate at \\(28.9\\%\\).\n\ndf_test[\"y_pred\"] = y_pred\ndf_test.groupby(pd.cut(df_test[\"person_age\"], [20,30,40,50,60,80]))[[\"y_pred\", \"loan_status\"]].mean().reset_index()\n\n\n\n\n\n\n\n\nperson_age\ny_pred\nloan_status\n\n\n\n\n0\n(20, 30]\n0.143839\n0.223499\n\n\n1\n(30, 40]\n0.088584\n0.208466\n\n\n2\n(40, 50]\n0.054545\n0.202429\n\n\n3\n(50, 60]\n0.029412\n0.289474\n\n\n4\n(60, 80]\n0.083333\n0.461538\n\n\n\n\n\n\n\nNext we can look at the predictions for default based on loan intent. Just as with the age groups, the predicted rates of default are much lower than the actual rates of default, although by varying amounts. Among the highest predicted rates of default are medical expenses with a predicted rate of \\(19.6\\%\\) compared to the actual default rate of \\(28.2\\%\\). This means it is significantly harder to get approved for a medical loan than other loans like education or venture loans, which are given predicted default rates of \\(6.3\\%\\) and \\(8.4\\%\\) respectively.\n\ndf_test.groupby(\"loan_intent\")[[\"y_pred\", \"loan_status\"]].mean().reset_index()\n\n\n\n\n\n\n\n\nloan_intent\ny_pred\nloan_status\n\n\n\n\n0\nDEBTCONSOLIDATION\n0.201327\n0.279497\n\n\n1\nEDUCATION\n0.062925\n0.167421\n\n\n2\nHOMEIMPROVEMENT\n0.118506\n0.246088\n\n\n3\nMEDICAL\n0.195713\n0.281553\n\n\n4\nPERSONAL\n0.119238\n0.219227\n\n\n5\nVENTURE\n0.084025\n0.145701\n\n\n\n\n\n\n\nLastly, we can look at income brackets. Borrowers that make less than \\(\\$40000\\) are the most likely to default, which the model reflects by making it the most difficult for these borrowers to access credit. As the income brackets increase, the model predicts lower and lower rates of default even though the actual rates of default seem to level off above \\(\\$70000\\).\n\ndf_test[\"y_pred\"] = y_pred\ndf_test.groupby(pd.cut(df_test[\"person_income\"], [0,40000,70000,100000, 150000,1000000]))[[\"y_pred\", \"loan_status\"]].mean().reset_index()\n\n\n\n\n\n\n\n\nperson_income\ny_pred\nloan_status\n\n\n\n\n0\n(0, 40000]\n0.236352\n0.370569\n\n\n1\n(40000, 70000]\n0.121875\n0.198972\n\n\n2\n(70000, 100000]\n0.068262\n0.119625\n\n\n3\n(100000, 150000]\n0.012797\n0.105008\n\n\n4\n(150000, 1000000]\n0.00495\n0.115044\n\n\n\n\n\n\n\n\n\nConclusion\nThroughout this post, we examine automated decision algorithms, which take advantage of threshold setting to determine if borrowers should be approved loans or not. By assigning variable costs to different kinds of errors, we define our optimal model as one that does not necessarily minimize all error, but minimizes a specific kind of error. Using this approach, we find parameters that can be used to maximize the profit for our bank. One of the ethical concerns with implementing an algorithm like this is that certain demographs are likely to be assigned higher or lower risk scores simply based on the demographic they are part of. As such, certain groups can be unfairly advantaged or disadvantaged in decision-making processes like approving loans. In the case of the bank, one such concern is that people seeking medical loans have higher rates of default and thus are denied credit by the model at higher rates. This brings up the question: is this fair? From a profit standpoint, this certaintly is fair as the bank wants to minimize the risk they take on when giving credit and riskier investments should be taken on less frequently. However, from a humanitarian standpoint, this is an unfair policy as medical expenses are often for life-or-death procedures and it could be considered inhumane to deny the borrower the money they need for such a procedure even if they are perhaps unable to pay it back. Thus it comes down to how you define fairness: Is fairness based on equal exchanges and kept promises or is it our responsibility as humans to provide for one another even when there is no direct benefit to us? It seems that there are some decisions that even the best machines cannot make."
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "This post examines the implementation of the logistic regression classifier. Using a linear model, we implement a gradient function that iteratively adjusts the weights used to score and classify a given dataset. We implement an additional term to our step function which allows for gradient descent with momentum. Using our implemented model, we perform a series of experiments that compare the efficiency of gradient descent and gradient descent with momentum, examine the effects of overfitting the model, and explore the efficacy of the model on empirical data. Based on these experiments, we can explore the capabilities and limits of logistic regression and gain a better understanding of gradient descent and one of its variations."
  },
  {
    "objectID": "posts/logistic-regression/index.html#vanilla-gradient-descent",
    "href": "posts/logistic-regression/index.html#vanilla-gradient-descent",
    "title": "Implementing Logistic Regression",
    "section": "Vanilla Gradient Descent",
    "text": "Vanilla Gradient Descent\nWe first perform logistic regression using vanilla gradient descent, meaning gradient descent where \\(\\beta = 0\\). We define the LogisticRegression model we implemented as well as the GradientDescentOptimizer, which will update the weights at each iteration. The training loop here is fairly simple, calling the step() function for a predetermined number of iterations. We also save the loss at each step so we can determine when the model has converged.\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n\nv_loss = []\nfor _ in range(3000):\n    l = LR.loss(X,y)\n    v_loss.append(l)\n    opt.step(X, y, alpha = 0.1, beta = 0)\n\nAfter \\(3000\\) iterations, we notice that the loss converges to a value close to \\(0\\). We can also observe that the our decision boundary (dictated by the optimal weights) perfectly divides the data into their correct clusters. Nice!\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 5))\nfig.suptitle('Logistic Regression')\nplot_results(LR, X, y, v_loss, ax)"
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "The Perceptron",
    "section": "",
    "text": "The perceptron algorithm is the simplest model of a neural network, designed to take multiple inputs and output a single output representing the class of the input in a binary classification problem. The model works by iteratively updating a set of weights until a loss of zero is achieved, indicating perfect classification accuracy. We implement the perceptron algorithm and test it on linearly separable, non-linearly separable, and multi-dimensional linearly separable data to test its capabilities. We also implement a minibatch version of the algorithm, which examines multiple rows of the data at each step and updates the weights using their average gradient. Using these experiments, we can determine the strengths and weaknesses of the perceptron algorithm, which we can use to inform improvements we make when developing further algorithms."
  },
  {
    "objectID": "posts/perceptron/index.html#the-gradient-function",
    "href": "posts/perceptron/index.html#the-gradient-function",
    "title": "The Perceptron",
    "section": "The Gradient Function",
    "text": "The Gradient Function\nThe learning capabilities of our perceptron largely occurs in the grad() function, where a gradient is calculated to update the weights of the algorithm (and thus improve the classification threshold). For each iteration of our algorithm, we pass a single row of the input data and its corresponding label into the algorithm. The algorithm calculates the update to the weights as such \\[\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\mathbf{1}[s_i(2y_i - 1) &lt; 0](2y_i - 1)\\mathbf{x}_i\\] where \\(i\\) represents the index of the row of data used and \\(s_i\\) represents the score of that row of data such that \\[s_i = \\langle w \\,, \\mathbf{x}_i\\rangle\\] with \\(w\\) representing the existing weights. At each step of the algorithm, the perceptron randomly selects an observation from the data and updates the weights if the observation is misclassified. Since our classifier selects groups based on if the score of a given observation is positive or negative, we can check if an observation is misclassified using the expression \\([s_i(2y_i - 1)&lt;0]\\). This expression converts each label \\(y\\) from values of \\(0\\) and \\(1\\) to \\(-1\\) and \\(1\\). This means that observations with negative scores should have a label of \\(-1\\) and observations with positive scores should have a label of \\(1\\). Because the product of two real numbers with opposite signs is always negative and the product of two real numbers with the same signs is positive, we will find that all correctly classified observations will have positive products when multiplying their scores and labels while misclassified observations will be negative. Thus, by evaluating the boolean expression above, we will multiply our gradient by \\(0\\) for correctly classified observations (no change is made to the weights) and \\(1\\) for misclassified observations. Our gradient is then determined by the values of the observation, which are added to the current weights if the real label is positive (\\(2y_i - 1\\) evaluates to \\(1\\)) and subtracted if the real label is negative."
  },
  {
    "objectID": "posts/perceptron/index.html#linearly-separable-data",
    "href": "posts/perceptron/index.html#linearly-separable-data",
    "title": "The Perceptron",
    "section": "1) Linearly Separable Data",
    "text": "1) Linearly Separable Data\nWe feed the same data from our initial trial into our new and improved perceptron and use the returned benchmark data to visualize the changes in the weights at each step where the dotted line represents the previous classification boundary and the solid line represents the updated boundary based on the selected point (shaded in black). The boundary improves at each step and while the loss does not always decrease, we can see that the orientation of the line continuously imrpoves indicating some sort of progress towards the optimal solution.\n\ntorch.manual_seed(18)\np, loss, ws, idxs = Percy_with_life360(X, y)\n# # set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nplot_progress(X, y, ws, loss, idxs, axarr)\nplt.tight_layout()\n\n\n\n\n\n\n\n\nWe also plot the loss for the entire training loop, which demonstrates that there is not always a direct improvement, largely because no changes are made any time that a correctly classified point is selected.\n\nplt.plot(loss)\nplt.title('Perceptron Loss on Linearly Separable Data')\nplt.ylabel('Loss')\nplt.xlabel('Iteration')\n\nText(0.5, 0, 'Iteration')"
  },
  {
    "objectID": "posts/perceptron/index.html#non-linearly-separable-data",
    "href": "posts/perceptron/index.html#non-linearly-separable-data",
    "title": "The Perceptron",
    "section": "2) Non-linearly Separable Data",
    "text": "2) Non-linearly Separable Data\nNow for a harder problem. Up to this point, there has always been a line that exists that could divide the two clusters with perfect accuracy. With this new data, this is no longer the case. We generate this data in the same way as the linearly separable data but increase the gaussian noise from \\(0.2\\) to \\(0.4\\), which causes the points from the two clusters to overlap.\n\ntorch.manual_seed(14)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nuX, uy = perceptron_data(300, 0.4, p_dims=2)\nplot_perceptron_data(uX, uy, ax)\n\n\n\n\n\n\n\n\nWhen visualizing Percy’s progress through the training loop, we notice that it never achieves a loss of \\(0\\). As we discussed earlier, this is because no solution exists that would achieve a perfect loss. Letting the perceptron algorithm run on this data will eventually achieve an optimal, but never perfect solution to the classification problem. In this case, we can achieve a very small loss, but it will never be \\(0\\). Because of the way the perceptron algorithm is designed, the training loop will continue indefinitely for this data, so it is important that we implemented the stopping condition which stops the training loop if there is no convergence after \\(1000\\) iterations.\n\ntorch.manual_seed(14)\np, loss, ws, idxs = Percy_with_life360(uX, uy)\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nplot_progress(uX, uy, ws, loss, idxs, axarr)\nplt.tight_layout()\n\n\n\n\n\n\n\n\nWhen we plot the loss, we can notice large rises and drops in the loss where each update to the weights are made. While we are able to get a fairly low loss (indicating high classification accuracy), we are required to cut off the training loop early since we know that the loop will never terminate. Because the loss does not converge, we can notice that the loss does not terminate at the lowest observed loss, revealing a weakness of the perceptron algorithm. Assigning an arbitrary cutoff like we did will cause the algorithm to cease at exactly \\(1000\\) iterations. This means we run the risk of stopping the algorithm in the middle of one of the large rises, meaning we don’t always get the lowest possible loss.\n\nplt.plot(loss)\nplt.title('Perceptron Loss Non-Linearly Separable Data')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\n\nText(0, 0.5, 'Loss')"
  },
  {
    "objectID": "posts/perceptron/index.html#dimensional-data",
    "href": "posts/perceptron/index.html#dimensional-data",
    "title": "The Perceptron",
    "section": "3) 8-Dimensional Data",
    "text": "3) 8-Dimensional Data\nOur final dataset contains eight dimensions instead of two, meaning we cannot visualize the change in the classification boundary as we did for the other datasets. However, we can still monitor the loss. When we do, we notice that the loss converges, indicating that the algorithm does in fact work for multi-dimensional data and that this particular dataset is linearly separable. Nice!\n\ntorch.manual_seed(16)\nmX, my = perceptron_data(300, 0.2, p_dims=8)\np, loss, ws, idxs = Percy_with_life360(mX, my)\nplt.plot(loss)\nplt.title('Perceptron Loss on 8-dimensional Data')\nplt.xlabel('Step')\nplt.ylabel('Loss')\n\nText(0, 0.5, 'Loss')"
  }
]